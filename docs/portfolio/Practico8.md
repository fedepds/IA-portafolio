# ğŸ“š PrÃ¡ctica 8 â€“ Backpropagation y Optimizadores
En esta prÃ¡ctica exploramos **MLPs aplicados a datasets de imÃ¡genes** (MNIST, Fashion-MNIST, CIFAR-10 y CIFAR-100), evaluando cÃ³mo distintos hiperparÃ¡metros y tÃ©cnicas afectan el rendimiento.

---

## ğŸ¯ Objetivos
- Aplicar **backpropagation** en redes densas (MLP).
- Explorar **arquitecturas** (profundidad y ancho).
- Comparar **funciones de activaciÃ³n**: ReLU, GELU, tanh.
- Evaluar **regularizaciÃ³n**: Dropout, L2, BatchNormalization.
- Probar **inicializadores de pesos**: HeNormal, GlorotUniform.
- Probar distintos **optimizadores**: Adam, SGD, RMSprop, AdamW.
- Usar **callbacks** (EarlyStopping, ReduceLROnPlateau, TensorBoard).
- Analizar resultados y proponer conclusiones.

---

## ğŸ”¬ MetodologÃ­a
1. **Preprocesamiento**:  
   - NormalizaciÃ³n de imÃ¡genes a rango [-1,1].  
   - Aplanado de 28Ã—28 y 32Ã—32Ã—3 â†’ vectores para MLP.  
   - Split: train, validaciÃ³n (10%), test.  

2. **Arquitecturas probadas**:  
   - 1â€“3 capas, 32â€“256 neuronas.  
   - Ejemplos: `[64]`, `[128, 64]`, `[256, 128, 64]`.  

3. **Condiciones controladas**:  
   - Se modificÃ³ **un hiperparÃ¡metro a la vez**.  
   - Cada entrenamiento: 5â€“10 Ã©pocas, batch size en {32, 64}.  

---

## ğŸ“Š Resultados principales

### ğŸ”¹ Arquitecturas
- **Redes muy pequeÃ±as** (`[32]`) â†’ baja capacidad, accuracy < 40% en CIFAR-10.  
- **MÃ¡s capas/neuronas** (ej: `[128,64]`) â†’ mejor rendimiento en validaciÃ³n.  
- A partir de cierto punto, aumentar neuronas generÃ³ **sobreajuste**.  

### ğŸ”¹ Activaciones
- **ReLU** â†’ mÃ¡s estable y rÃ¡pida, mejor accuracy en general.  
- **GELU** â†’ resultados similares a ReLU pero mÃ¡s suaves.  
- **tanh** â†’ se quedÃ³ atrÃ¡s en datasets grandes (CIFAR-100).  

### ğŸ”¹ RegularizaciÃ³n
- **Dropout=0.2** â†’ redujo sobreajuste y mejorÃ³ generalizaciÃ³n.  
- **BatchNormalization** â†’ ayudÃ³ a estabilizar el entrenamiento.  
- **L2 regularization (1e-4)** â†’ Ãºtil, pero demasiado fuerte puede frenar el aprendizaje.  

### ğŸ”¹ Inicializadores
- **HeNormal** â†’ mÃ¡s adecuado con ReLU, mejor convergencia.  
- **GlorotUniform** â†’ buen rendimiento en general.  

### ğŸ”¹ Optimizadores
- **Adam (lr=1e-3)** â†’ buen balance entre rapidez y precisiÃ³n.  
- **SGD con momentum** â†’ mÃ¡s lento pero estable, sensible al LR.  
- **RMSprop** â†’ funcionÃ³ bien en datasets mÃ¡s complejos.  
- **AdamW** â†’ Ãºtil cuando se combina con decay (weight decay=1e-4).  

### ğŸ”¹ Callbacks
- **EarlyStopping** â†’ evitÃ³ entrenamientos innecesarios.  
- **ReduceLROnPlateau** â†’ permitiÃ³ recuperar accuracy en casos con LR muy alto.  
- **ModelCheckpoint** â†’ guardÃ³ siempre el mejor modelo.  

---

## ğŸ“ Conclusiones
- La **arquitectura importa**, pero agregar mÃ¡s neuronas no siempre mejora: llega un punto de sobreajuste.  
- **ReLU + HeNormal + Adam** fue la combinaciÃ³n mÃ¡s robusta.  
- La **regularizaciÃ³n ligera** (Dropout 0.2 + BatchNorm) ayudÃ³ a mejorar la generalizaciÃ³n.  
- En datasets simples (MNIST, Fashion-MNIST), incluso redes pequeÃ±as logran >90% accuracy.  
- En datasets complejos (CIFAR-100), un MLP se queda corto â†’ se justifica pasar a **redes convolucionales (CNNs)**.  

---

## ğŸ“Œ ReflexiÃ³n personal
Este ejercicio me permitiÃ³ entender:
- CÃ³mo cada **hiperparÃ¡metro** afecta al entrenamiento.  
- La importancia de **experimentar de manera sistemÃ¡tica** (un cambio a la vez).  
- Que el **MLP es limitado para visiÃ³n**, pero sirve como base para comprender *backpropagation, optimizadores y regularizaciÃ³n*.  

PrÃ³ximo paso: aplicar las mismas tÃ©cnicas con **CNNs** para mejorar la performance en datasets de imÃ¡genes mÃ¡s complejos.