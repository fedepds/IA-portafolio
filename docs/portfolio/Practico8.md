# Backpropagation y Optimizadores

En este proyecto explor√© **MLPs aplicados a datasets de im√°genes** (MNIST, Fashion-MNIST, CIFAR-10/100), demostrando mi comprensi√≥n profunda de:

- **Backpropagation**: El algoritmo fundamental que permite entrenar redes neuronales profundas.
- **Optimizadores avanzados**: Comparaci√≥n emp√≠rica de Adam, SGD, RMSprop y AdamW.
- **Arquitecturas neuronales**: Experimentaci√≥n sistem√°tica con profundidad y ancho de capas.
- **Regularizaci√≥n**: Implementaci√≥n de Dropout, L2 y BatchNormalization.
- **Callbacks**: Uso de EarlyStopping, ReduceLROnPlateau y TensorBoard para entrenamiento eficiente.

Realic√© experimentos controlados modificando un hiperpar√°metro a la vez para aislar su impacto en el rendimiento.

--- (MNIST, Fashion-MNIST, CIFAR-10 y CIFAR-100), evaluando c√≥mo distintos hiperpar√°metros y t√©cnicas afectan el rendimiento.

---

## üéØ Objetivos
- Aplicar **backpropagation** en redes densas (MLP).
- Explorar **arquitecturas** (profundidad y ancho).
- Comparar **funciones de activaci√≥n**: ReLU, GELU, tanh.
- Evaluar **regularizaci√≥n**: Dropout, L2, BatchNormalization.
- Probar **inicializadores de pesos**: HeNormal, GlorotUniform.
- Probar distintos **optimizadores**: Adam, SGD, RMSprop, AdamW.
- Usar **callbacks** (EarlyStopping, ReduceLROnPlateau, TensorBoard).
- Analizar resultados y proponer conclusiones.

---

## üî¨ Metodolog√≠a
1. **Preprocesamiento**:  
   - Normalizaci√≥n de im√°genes a rango [-1,1].  
   - Aplanado de 28√ó28 y 32√ó32√ó3 ‚Üí vectores para MLP.  
   - Split: train, validaci√≥n (10%), test.  

2. **Arquitecturas probadas**:  
   - 1‚Äì3 capas, 32‚Äì256 neuronas.  
   - Ejemplos: `[64]`, `[128, 64]`, `[256, 128, 64]`.  

3. **Condiciones controladas**:  
   - Se modific√≥ **un hiperpar√°metro a la vez**.  
   - Cada entrenamiento: 5‚Äì10 √©pocas, batch size en {32, 64}.  

---

## üìä Resultados principales

### üîπ Arquitecturas
- **Redes muy peque√±as** (`[32]`) ‚Üí baja capacidad, accuracy < 40% en CIFAR-10.  
- **M√°s capas/neuronas** (ej: `[128,64]`) ‚Üí mejor rendimiento en validaci√≥n.  
- A partir de cierto punto, aumentar neuronas gener√≥ **sobreajuste**.  

### üîπ Activaciones
- **ReLU** ‚Üí m√°s estable y r√°pida, mejor accuracy en general.  
- **GELU** ‚Üí resultados similares a ReLU pero m√°s suaves.  
- **tanh** ‚Üí se qued√≥ atr√°s en datasets grandes (CIFAR-100).  

### üîπ Regularizaci√≥n
- **Dropout=0.2** ‚Üí redujo sobreajuste y mejor√≥ generalizaci√≥n.  
- **BatchNormalization** ‚Üí ayud√≥ a estabilizar el entrenamiento.  
- **L2 regularization (1e-4)** ‚Üí √∫til, pero demasiado fuerte puede frenar el aprendizaje.  

### üîπ Inicializadores
- **HeNormal** ‚Üí m√°s adecuado con ReLU, mejor convergencia.  
- **GlorotUniform** ‚Üí buen rendimiento en general.  

### üîπ Optimizadores
- **Adam (lr=1e-3)** ‚Üí buen balance entre rapidez y precisi√≥n.  
- **SGD con momentum** ‚Üí m√°s lento pero estable, sensible al LR.  
- **RMSprop** ‚Üí funcion√≥ bien en datasets m√°s complejos.  
- **AdamW** ‚Üí √∫til cuando se combina con decay (weight decay=1e-4).  

### üîπ Callbacks
- **EarlyStopping** ‚Üí evit√≥ entrenamientos innecesarios.  
- **ReduceLROnPlateau** ‚Üí permiti√≥ recuperar accuracy en casos con LR muy alto.  
- **ModelCheckpoint** ‚Üí guard√≥ siempre el mejor modelo.  

---

## üìù Conclusiones
- La **arquitectura importa**, pero agregar m√°s neuronas no siempre mejora: llega un punto de sobreajuste.  
- **ReLU + HeNormal + Adam** fue la combinaci√≥n m√°s robusta.  
- La **regularizaci√≥n ligera** (Dropout 0.2 + BatchNorm) ayud√≥ a mejorar la generalizaci√≥n.  
- En datasets simples (MNIST, Fashion-MNIST), incluso redes peque√±as logran >90% accuracy.  
- En datasets complejos (CIFAR-100), un MLP se queda corto ‚Üí se justifica pasar a **redes convolucionales (CNNs)**.  

---

## üìå Reflexi√≥n personal
Este ejercicio me permiti√≥ entender:
- C√≥mo cada **hiperpar√°metro** afecta al entrenamiento.  
- La importancia de **experimentar de manera sistem√°tica** (un cambio a la vez).  
- Que el **MLP es limitado para visi√≥n**, pero sirve como base para comprender *backpropagation, optimizadores y regularizaci√≥n*.  

Pr√≥ximo paso: aplicar las mismas t√©cnicas con **CNNs** para mejorar la performance en datasets de im√°genes m√°s complejos.

---

## üìì Notebook

**[Abrir en Google Colab](https://colab.research.google.com/github/fedepds/IA-portafolio/blob/main/docs/portfolio/UT2/Practica8.ipynb)**