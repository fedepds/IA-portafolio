# Feature Engineering y Modelo Base

## üéØ Descripci√≥n
En este proyecto extend√≠ el an√°lisis exploratorio del dataset Titanic, demostrando mis habilidades en:
1. T√©cnicas avanzadas de imputaci√≥n de datos faltantes con enfoque inteligente.
2. Ingenier√≠a de caracter√≠sticas (feature engineering) para extraer informaci√≥n relevante de variables existentes.
3. Entrenamiento y evaluaci√≥n de modelos predictivos, estableciendo l√≠neas base robustas.

---

## üìä Dataset
- **Fuente**: [Titanic - Kaggle](https://www.kaggle.com/c/titanic)
- **Variable Importante**: `Survived` (0 = no sobrevivi√≥, 1 = sobrevivi√≥).

---

## üîß Metodolog√≠a Aplicada

### 1. Preprocesamiento Inteligente de Datos
Implement√© estrategias espec√≠ficas de imputaci√≥n seg√∫n la naturaleza de cada variable:
- **Embarked**: Imput√© valores faltantes con la moda (valor m√°s frecuente), m√©todo apropiado para variables categ√≥ricas.
- **Fare**: Utilic√© la mediana para evitar distorsi√≥n por valores extremos (outliers).
- **Age**: Apliqu√© imputaci√≥n estrat√©gica calculando la mediana por grupos (`Sex` y `Pclass`), logrando estimaciones m√°s precisas y contextualizadas.

### 2. Ingenier√≠a de Caracter√≠sticas (Feature Engineering)
Cre√© nuevas variables derivadas para capturar patrones ocultos en los datos:
- **`FamilySize`**: Combin√© `SibSp` + `Parch` + 1 para cuantificar el tama√±o del grupo familiar.
- **`IsAlone`**: Variable binaria indicando si el pasajero viajaba solo (1) o acompa√±ado (0).
- **`Title`**: Extra√≠e t√≠tulos sociales de la columna `Name` (Mr., Mrs., Miss., etc.) para capturar estatus social y patrones demogr√°ficos, agrupando t√≠tulos raros en una categor√≠a unificada.

Estas variables capturaron hip√≥tesis de dominio: las familias pod√≠an tener ventajas/desventajas en supervivencia, y el estatus social influy√≥ en el acceso a botes salvavidas.

### 3. Transformaci√≥n de Variables Categ√≥ricas
Apliqu√© **one-hot encoding** mediante `pd.get_dummies` para convertir variables categ√≥ricas (`Sex`, `Embarked`, `Title`) en representaciones num√©ricas binarias procesables por modelos de machine learning.

### 4. Modelado y Evaluaci√≥n
Entrene y compar√© dos aproximaciones:
- **DummyClassifier (Baseline)**: Establec√≠ una l√≠nea base prediciendo siempre la clase mayoritaria. Utilic√© `class_weight="balanced"` para manejar desbalance de clases.
- **Regresi√≥n Log√≠stica**: Implement√© un modelo de clasificaci√≥n supervisada, validando la mejora frente al baseline.
- Utilic√© `train_test_split` para divisi√≥n apropiada de datos y evitar sobreajuste.

---

## üìà Resultados

- **Baseline (DummyClassifier)**: Acc ‚âà *0.62*.  
- **Regresi√≥n Log√≠stica**: Acc ‚âà *0.79*.  

üìå El modelo de regresi√≥n log√≠stica **supera claramente al baseline**, lo que confirma que las features creadas y el preprocesamiento aportan informaci√≥n valiosa.

---

## üîç An√°lisis de la matriz de confusi√≥n
- **Falsos positivos**: casos donde el modelo predijo supervivencia pero en realidad no ocurri√≥.  
- **Falsos negativos**: casos donde el modelo predijo no supervivencia pero la persona s√≠ sobrevivi√≥.  
üëâ En este contexto, los **falsos negativos son m√°s graves**, porque implican ‚Äúno salvar‚Äù a alguien que s√≠ pod√≠a sobrevivir.  

El modelo tiende a equivocarse m√°s con los **no sobrevivientes**.

---

## üöÄ Reflexi√≥n y mejoras
- El **feature engineering** aport√≥ mucho valor: especialmente `Title` y `FamilySize`.  
- A futuro, se podr√≠an crear nuevas variables a partir de:
  - La cabina (`Cabin`) ‚Üí ubicaci√≥n en el barco.
  - El billete (`Ticket`) ‚Üí posibles grupos de viaje.  

Esto abrir√≠a la puerta a modelos m√°s complejos como **√°rboles de decisi√≥n o random forest**.

---

## üìì Notebook

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fedepds/IA-portafolio/blob/main/docs/portfolio/UT1/Practico2/Practico2Pizarro.ipynb)

[Ver Notebook en GitHub](UT1/Practico2/Practico2Pizarro.ipynb)
