# Feature Engineering y Modelo Base

## ğŸ¯ DescripciÃ³n
En este proyecto extendÃ­ el anÃ¡lisis exploratorio del dataset Titanic, demostrando mis habilidades en:
1. TÃ©cnicas avanzadas de imputaciÃ³n de datos faltantes con enfoque inteligente.
2. IngenierÃ­a de caracterÃ­sticas (feature engineering) para extraer informaciÃ³n relevante de variables existentes.
3. Entrenamiento y evaluaciÃ³n de modelos predictivos, estableciendo lÃ­neas base robustas.

---

## ğŸ“Š Dataset
- **Fuente**: [Titanic - Kaggle](https://www.kaggle.com/c/titanic)
- **Variable Importante**: `Survived` (0 = no sobreviviÃ³, 1 = sobreviviÃ³).

---

## ğŸ”§ MetodologÃ­a Aplicada

### 1. Preprocesamiento Inteligente de Datos
ImplementÃ© estrategias especÃ­ficas de imputaciÃ³n segÃºn la naturaleza de cada variable:
- **Embarked**: ImputÃ© valores faltantes con la moda (valor mÃ¡s frecuente), mÃ©todo apropiado para variables categÃ³ricas.
- **Fare**: UtilicÃ© la mediana para evitar distorsiÃ³n por valores extremos (outliers).
- **Age**: ApliquÃ© imputaciÃ³n estratÃ©gica calculando la mediana por grupos (`Sex` y `Pclass`), logrando estimaciones mÃ¡s precisas y contextualizadas.

### 2. IngenierÃ­a de CaracterÃ­sticas (Feature Engineering)
CreÃ© nuevas variables derivadas para capturar patrones ocultos en los datos:
- **`FamilySize`**: CombinÃ© `SibSp` + `Parch` + 1 para cuantificar el tamaÃ±o del grupo familiar.
- **`IsAlone`**: Variable binaria indicando si el pasajero viajaba solo (1) o acompaÃ±ado (0).
- **`Title`**: ExtraÃ­e tÃ­tulos sociales de la columna `Name` (Mr., Mrs., Miss., etc.) para capturar estatus social y patrones demogrÃ¡ficos, agrupando tÃ­tulos raros en una categorÃ­a unificada.

Estas variables capturaron hipÃ³tesis de dominio: las familias podÃ­an tener ventajas/desventajas en supervivencia, y el estatus social influyÃ³ en el acceso a botes salvavidas.

### 3. TransformaciÃ³n de Variables CategÃ³ricas
ApliquÃ© **one-hot encoding** mediante `pd.get_dummies` para convertir variables categÃ³ricas (`Sex`, `Embarked`, `Title`) en representaciones numÃ©ricas binarias procesables por modelos de machine learning.

### 4. Modelado y EvaluaciÃ³n
Entrene y comparÃ© dos aproximaciones:
- **DummyClassifier (Baseline)**: EstablecÃ­ una lÃ­nea base prediciendo siempre la clase mayoritaria. UtilicÃ© `class_weight="balanced"` para manejar desbalance de clases.
- **RegresiÃ³n LogÃ­stica**: ImplementÃ© un modelo de clasificaciÃ³n supervisada, validando la mejora frente al baseline.
- UtilicÃ© `train_test_split` para divisiÃ³n apropiada de datos y evitar sobreajuste.

---

## ğŸ“ˆ Resultados

- **Baseline (DummyClassifier)**: Acc â‰ˆ *0.62*.  
- **RegresiÃ³n LogÃ­stica**: Acc â‰ˆ *0.79*.  

ğŸ“Œ El modelo de regresiÃ³n logÃ­stica **supera claramente al baseline**, lo que confirma que las features creadas y el preprocesamiento aportan informaciÃ³n valiosa.

---

## ğŸ” AnÃ¡lisis de la matriz de confusiÃ³n
- **Falsos positivos**: casos donde el modelo predijo supervivencia pero en realidad no ocurriÃ³.  
- **Falsos negativos**: casos donde el modelo predijo no supervivencia pero la persona sÃ­ sobreviviÃ³.  
ğŸ‘‰ En este contexto, los **falsos negativos son mÃ¡s graves**, porque implican â€œno salvarâ€ a alguien que sÃ­ podÃ­a sobrevivir.  

El modelo tiende a equivocarse mÃ¡s con los **no sobrevivientes**.

---

## ğŸš€ ReflexiÃ³n y mejoras
- El **feature engineering** aportÃ³ mucho valor: especialmente `Title` y `FamilySize`.  
- A futuro, se podrÃ­an crear nuevas variables a partir de:
  - La cabina (`Cabin`) â†’ ubicaciÃ³n en el barco.
  - El billete (`Ticket`) â†’ posibles grupos de viaje.  

Esto abrirÃ­a la puerta a modelos mÃ¡s complejos como **Ã¡rboles de decisiÃ³n o random forest**.

---

## ğŸ““ Notebook

[Ver Notebook Completo](UT1/Practico2/Practico2Pizarro.ipynb)
