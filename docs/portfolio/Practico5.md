# Validaci贸n Cruzada y Selecci贸n de Modelos

##  Descripci贸n
En este proyecto apliqu茅 t茅cnicas avanzadas de Machine Learning para un problema de alto impacto social: predicci贸n de abandono estudiantil. Demostr茅 mis habilidades en:
1. Prevenci贸n de **data leakage** mediante pipelines estructurados.
2. Implementaci贸n de **validaci贸n cruzada (K-Fold)** para estimaciones robustas de rendimiento.
3. Comparaci贸n sistem谩tica de m煤ltiples algoritmos de clasificaci贸n.
4. An谩lisis de estabilidad y variabilidad de modelos para selecci贸n informada.

---

##  Contexto y Dataset
- **Problema**: predecir **abandono estudiantil** y **茅xito acad茅mico** en educaci贸n superior.  
- **Objetivo aplicado**: identificar estudiantes en riesgo para implementar estrategias de apoyo.  
- **Variables**: 36 caracter铆sticas (demogr谩ficas, acad茅micas, socioecon贸micas).  
- **Valor**: mejorar la **retenci贸n estudiantil** y reducir la tasa de abandono.  

---

##  Proceso

### 1. Preparaci贸n de datos
- Se construyeron **pipelines** para estandarizar el preprocesamiento y evitar fugas de datos.  
- Se aplic贸 divisi贸n estratificada de los datos para mantener balance entre clases.

### 2. Validaci贸n cruzada
- Se utiliz贸 **K-Fold Cross Validation** para obtener estimaciones m谩s confiables del rendimiento de los modelos.  
- Esta t茅cnica permiti贸 observar no solo la media de la m茅trica, sino tambi茅n la **variabilidad entre folds**.

### 3. Modelos evaluados
- **DummyClassifier**  
  - Modelo base que predice siempre la clase m谩s frecuente.  
  - Sirve como referencia m铆nima: cualquier modelo 煤til debe superarlo.  

- **Regresi贸n Log铆stica**  
  - Modelo lineal para clasificaci贸n.  
  - Estima la probabilidad de pertenecer a una clase mediante la funci贸n log铆stica.  
  - Sencillo, interpretable y suele ser muy robusto.  

- **Random Forest**  
  - Conjunto de m煤ltiples 谩rboles de decisi贸n entrenados en subconjuntos de datos.  
  - Reduce la varianza de un 谩rbol individual y mejora la generalizaci贸n.  

- **Gradient Boosting (ej. XGBoost/LightGBM)**  
  - Algoritmo de ensamble que construye 谩rboles de forma secuencial, cada uno corrigiendo los errores del anterior.  
  - Tiende a ser muy preciso, aunque m谩s sensible a par谩metros y con mayor riesgo de sobreajuste.  

---

##  Resultados

- El **DummyClassifier** sirvi贸 como referencia m铆nima.  
- La **Regresi贸n Log铆stica** mostr贸 un rendimiento s贸lido y estable.  
- **Random Forest** y **Gradient Boosting** presentaron buena performance, pero con mayor varianza en algunos folds.  
- Seg煤n los boxplots de validaci贸n cruzada, la **Logistic Regression fue la m谩s estable**, mientras que las diferencias en bigotes de otros modelos pudieron deberse a outliers o folds at铆picos.

---

##  An谩lisis
- La **estabilidad del modelo** es tan importante como la m茅trica promedio: un modelo con menor varianza ofrece predicciones m谩s confiables.  
- En este caso, la Regresi贸n Log铆stica result贸 competitiva y consistente, lo que la hace un buen candidato para producci贸n.  

---

##  Reflexi贸n y mejoras
- Se recomienda probar **nuevas features derivadas** (interacciones, combinaciones de variables socioecon贸micas y acad茅micas).  
- Explorar m茅todos de **regularizaci贸n** (Ridge, Lasso) en la regresi贸n log铆stica.  
- Implementar t茅cnicas de **balanceo de clases** (SMOTE, undersampling) si la variable objetivo est谩 desbalanceada.  
- Comparar con modelos m谩s complejos como **Redes Neuronales** o **Stacking** para ensambles.  

---

##  Notebook

**[Abrir en Google Colab](https://colab.research.google.com/github/fedepds/IA-portafolio/blob/main/docs/portfolio/UT1/Practico5/TA5.ipynb)**
