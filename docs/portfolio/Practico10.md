# Data Augmentation y XAI con Transfer Learning

En este proyecto apliqu√© **Transfer Learning** a problemas complejos de clasificaci√≥n multi-clase, integrando t√©cnicas avanzadas de explicabilidad:

- **Data Augmentation**: Pipeline robusto con transformaciones m√∫ltiples (flip, rotation, zoom, brightness, contrast) para simular variabilidad real.
- **Transfer Learning con Fine-tuning en dos etapas**: EfficientNetB0 pre-entrenado en ImageNet, primero congelado y luego fine-tuning completo.
- **Explicabilidad (XAI)**: Implementaci√≥n de GradCAM e Integrated Gradients para entender y auditar las decisiones del modelo.
- **Diagn√≥stico sistem√°tico**: B√∫squeda autom√°tica de errores en predicciones y an√°lisis visual de por qu√© el modelo falla.
- **Generalizaci√≥n multi-dominio**: Validaci√≥n en dos datasets (Oxford Flowers102 y Stanford Dogs) para demostrar transferibilidad de la metodolog√≠a.

Aprend√≠ que la explicabilidad no es un "extra" sino una herramienta cr√≠tica de debugging, auditor√≠a y generaci√≥n de confianza en modelos de producci√≥n.

---

## Objetivos

- Aplicar Transfer Learning (EfficientNetB0) para clasificaci√≥n multi-clase con datasets complejos.
- Implementar pipeline de Data Augmentation para simular variaciones de captura real.
- Experimentar con estrategia de fine-tuning en dos etapas (congelado + descongelado).
- Implementar callbacks (EarlyStopping) para prevenir overfitting y guardar el mejor modelo.
- Implementar GradCAM e Integrated Gradients para auditar decisiones del modelo.
- Diagnosticar errores sistem√°ticamente usando XAI para identificar causas de predicciones incorrectas.
- Validar metodolog√≠a en m√∫ltiples dominios (flores y perros) para demostrar generalizaci√≥n.

---

## Metodolog√≠a

### Datasets Utilizados

#### 1. Oxford Flowers102
- **Descripci√≥n**: 102 especies de flores comunes en el Reino Unido
- **Tama√±o**: ~8,189 im√°genes (train + test)
- **Subset usado**: 5,000 train / 1,000 test (para iteraci√≥n r√°pida)
- **Desaf√≠o**: Alta similitud inter-clase, variabilidad en fondos y condiciones de iluminaci√≥n

#### 2. Stanford Dogs  
- **Descripci√≥n**: 120 razas de perros
- **Tama√±o**: ~20,580 im√°genes (train + test)
- **Subset usado**: 8,000 train / 1,000 test
- **Desaf√≠o**: Razas con caracter√≠sticas visuales muy similares, variabilidad en poses y contextos

### Preprocesamiento

- Carga con TensorFlow Datasets (TFDS) usando `as_supervised=True`
- Redimensionado a 224√ó224 (input est√°ndar de EfficientNet)
- Normalizaci√≥n con `preprocess_input` de EfficientNet (rangos espec√≠ficos para ImageNet)
- Split estrat√©gico: subsets para experimentaci√≥n r√°pida

### Arquitectura (Transfer Learning en 2 Etapas)

**Base**: EfficientNetB0 pre-entrenado en ImageNet

**Etapa 1 - Entrenamiento de cabezal (base congelada)**:
- `base_model.trainable = False`
- Cabezal: GlobalAveragePooling2D ‚Üí Dropout(0.3) ‚Üí Dense(num_classes, softmax)
- Optimizador: Adam (lr por defecto)
- Epochs: 7 con EarlyStopping(patience=3)

**Etapa 2 - Fine-tuning completo**:
- `base_model.trainable = True` (descongelar todas las capas)
- Re-compilaci√≥n con learning rate bajo: Adam(lr=1e-5)
- Epochs: 10 con EarlyStopping(patience=3)
- Callback `restore_best_weights=True` para recuperar el mejor modelo

### Pipeline de Augmentation

Capas de Keras aplicadas durante entrenamiento:
- `RandomFlip("horizontal")` - flip horizontal para simetr√≠a
- `RandomRotation(0.15)` - rotaci√≥n ¬±15% para invarianza rotacional
- `RandomZoom(0.2)` - zoom ¬±20% para simular distancias variables
- `RandomBrightness(0.2)` - variaci√≥n de brillo ¬±20%
- `RandomContrast(0.2)` - variaci√≥n de contraste ¬±20%

### Entrenamiento y Callbacks

- **Optimizador**: Adam (lr=1e-3 etapa 1, lr=1e-5 etapa 2)
- **Loss**: sparse_categorical_crossentropy (labels como enteros)
- **Callbacks**: EarlyStopping con `monitor='val_loss'`, `patience=3`, `restore_best_weights=True`
- **Estrategia**: Entrenamiento en 2 fases evita catastrophic forgetting y optimiza convergencia

### An√°lisis de Explicabilidad (XAI)

**GradCAM (Gradient-weighted Class Activation Mapping)**:
- Aplicado sobre √∫ltima capa convolucional (detecci√≥n autom√°tica)
- Generaci√≥n de heatmaps mostrando regiones de atenci√≥n del modelo
- Visualizaci√≥n en 3 vistas: original, heatmap, overlay

**Integrated Gradients**:
- Baseline: imagen negra (tensor de ceros)
- 50 pasos de interpolaci√≥n entre baseline y imagen real
- Atribuci√≥n pixel-wise m√°s precisa que GradCAM
- Visualizaci√≥n con colormap 'inferno' para destacar p√≠xeles importantes

**Diagn√≥stico de Errores**:
- B√∫squeda autom√°tica de predicciones incorrectas en test set
- Aplicaci√≥n de GradCAM sobre errores para an√°lisis de causa ra√≠z
- Clasificaci√≥n de tipos de error: correlaci√≥n espuria (fondo), confusi√≥n honesta (similitud visual), baja confianza

---

## Resultados

### Experimento 1: Oxford Flowers102

**Rendimiento**:
- Val accuracy m√°ximo: ~81.5% (√©poca 6)
- Test accuracy final: ~78.8%
- Train accuracy final: ~98.2%

**An√°lisis de Overfitting**:
- Brecha train-val: ~20 puntos porcentuales
- Val loss m√≠nimo en √©poca 6 (~0.7544), luego aument√≥
- EarlyStopping activado para prevenir degradaci√≥n adicional

**GradCAM - Predicciones correctas**:
- Atenci√≥n focalizada en p√©talos, estambres y centro reproductivo de flores
- Ignorancia de fondo y follaje ‚Üí validaci√≥n de que el modelo no usa correlaciones espurias
- Confianza alta en regiones discriminativas correctas

**Integrated Gradients**:
- Atribuciones m√°s finas en estructuras florales espec√≠ficas
- Mayor precisi√≥n pixel-wise que GradCAM
- √ötil para an√°lisis detallado de caracter√≠sticas discriminativas

**Diagn√≥stico de Errores**:
- Errores t√≠picos: confusi√≥n entre flores con colores/formas similares
- GradCAM mostr√≥ que en algunos errores el modelo miraba partes correctas pero insuficientes
- Casos de baja confianza correlacionados con fondos complejos

### Experimento 2: Stanford Dogs

**Rendimiento**:
- Mejora notable con fine-tuning en 2 etapas
- EarlyStopping efectivo para prevenir overfitting
- Test accuracy superior a Flowers102 (razas de perros tienen caracter√≠sticas m√°s distintivas)

**Validaci√≥n de Generalizaci√≥n**:
- Mismo pipeline funcion√≥ exitosamente en dominio diferente
- GradCAM mostr√≥ atenci√≥n en caracter√≠sticas caninas relevantes: orejas, hocico, pelaje
- Metodolog√≠a XAI consistente y reproducible across dominios

---

## Conclusiones

### T√©cnicas

- **Transfer Learning con EfficientNetB0** es altamente efectivo para clasificaci√≥n multi-clase compleja (102-120 clases)
- **Fine-tuning en 2 etapas** (congelado ‚Üí descongelado con lr bajo) evita catastrophic forgetting y optimiza convergencia
- **EarlyStopping con restore_best_weights** es cr√≠tico para evitar overfitting y recuperar el mejor modelo autom√°ticamente
- **Data Augmentation** mejora robustez pero no elimina completamente el overfitting cuando hay limitaci√≥n de datos
- T√©cnicas avanzadas recomendadas: Mixup/CutMix, regularizaci√≥n L2, mayor volumen de datos

### Explicabilidad (XAI)

- **GradCAM** es herramienta esencial de debugging: identifica si el modelo usa caracter√≠sticas correctas o correlaciones espurias
- **Integrated Gradients** proporciona atribuciones m√°s precisas, √∫til para an√°lisis detallado
- **Diagn√≥stico de errores con XAI** permite clasificar tipos de fallo y priorizar mejoras
- XAI transforma modelos de "caja negra" a sistemas auditables y confiables

### Aplicaci√≥n Pr√°ctica

**Generaci√≥n de Confianza**:
- Visualizaciones XAI demuestran a usuarios/expertos que el modelo funciona por razones correctas
- Validaci√≥n de que no hay "trampas" (ej: mirar fondos en lugar de sujeto)
- Cr√≠tico para adopci√≥n en contextos profesionales (bot√°nica, veterinaria)

**Mitigaci√≥n de Riesgos**:
- Modelo sin explicabilidad presenta riesgos: confusi√≥n peligrosa (flores venenosas, diagn√≥sticos incorrectos)
- XAI permite auditor√≠a pre-deployment y monitoreo continuo
- Facilita debugging sistem√°tico vs trial-and-error

### Generalizaci√≥n Multi-Dominio

- Metodolog√≠a validada exitosamente en 2 dominios (flores, perros)
- Pipeline transferible a otros problemas de clasificaci√≥n visual
- T√©cnicas XAI consistentes regardless del dominio espec√≠fico

---

## Reflexi√≥n Personal

- Aprend√≠ que accuracy es insuficiente: necesitamos entender **c√≥mo** el modelo llega a sus predicciones
- GradCAM revel√≥ que algunos modelos "adivinan bien" mirando cosas incorrectas ‚Üí √©xito superficial pero modelo fr√°gil
- La diferencia entre "modelo que funciona" y "modelo en el que podemos confiar" radica en la explicabilidad
- Fine-tuning en 2 etapas demostr√≥ ser superior a fine-tuning directo o congelamiento permanente
- EarlyStopping es herramienta cr√≠tica pero requiere configuraci√≥n cuidadosa (m√©trica, patience, restore_best_weights)

---

## Exploraciones Futuras

**Mejoras de Rendimiento**:
- Implementar Mixup/CutMix para regularizaci√≥n adicional durante entrenamiento
- Probar arquitecturas m√°s grandes (EfficientNetB3-B7) con mayor capacidad
- Aumentar volumen de datos (usar dataset completo + augmentation offline)

**T√©cnicas Avanzadas XAI**:
- LIME (Local Interpretable Model-agnostic Explanations) para comparaci√≥n
- Saliency Maps y Attention Rollout para an√°lisis adicional
- Quantitative evaluation de mapas de atenci√≥n vs ground truth annotations

**MLOps**:
- Implementar ModelCheckpoint para tracking de m√∫ltiples checkpoints
- A/B testing de variantes de modelo en producci√≥n
- Pipeline de monitoreo continuo de explicabilidad (drift detection)

---

## üìì Notebook

**[Abrir en Google Colab](https://colab.research.google.com/github/fedepds/IA-portafolio/blob/main/docs/portfolio/UT3/Practico10.ipynb)**
