{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNfjgttQBMVp",
        "outputId": "8695aa58-dee1-4616-8752-93823234cb3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain>=0.2.11 in /usr/local/lib/python3.12/dist-packages (1.0.5)\n",
            "Requirement already satisfied: langchain-core>=0.2.33 in /usr/local/lib/python3.12/dist-packages (1.0.4)\n",
            "Requirement already satisfied: langchain-community>=0.2.11 in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain-openai>=0.2.1 in /usr/local/lib/python3.12/dist-packages (1.0.2)\n",
            "Requirement already satisfied: langsmith>=0.1.97 in /usr/local/lib/python3.12/dist-packages (0.4.42)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain>=0.2.11) (1.0.3)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain>=0.2.11) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.2.33) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.2.33) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.2.33) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.2.33) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.2.33) (4.15.0)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.2.11) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.2.11) (2.0.44)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.2.11) (2.32.5)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.2.11) (3.13.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.2.11) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.2.11) (2.12.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.2.11) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.2.11) (2.0.2)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai>=0.2.1) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai>=0.2.1) (0.12.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.97) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.97) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.97) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.97) (0.25.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.11) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.11) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.11) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.11) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.11) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.11) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.11) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community>=0.2.11) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community>=0.2.11) (0.9.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.97) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.97) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.97) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.97) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.97) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.2.33) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community>=0.2.11) (1.0.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain>=0.2.11) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain>=0.2.11) (1.0.2)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain>=0.2.11) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain>=0.2.11) (3.6.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai>=0.2.1) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai>=0.2.1) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai>=0.2.1) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai>=0.2.1) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.2.11) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.2.11) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.2.11) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community>=0.2.11) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community>=0.2.11) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community>=0.2.11) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community>=0.2.11) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai>=0.2.1) (2024.11.6)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain>=0.2.11) (1.12.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community>=0.2.11) (1.1.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Collecting chromadb\n",
            "  Using cached chromadb-1.3.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\n",
            "Collecting tavily-python\n",
            "  Using cached tavily_python-0.7.12-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting duckduckgo-search\n",
            "  Using cached duckduckgo_search-8.1.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.10)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.38.0)\n",
            "Collecting posthog<6.0.0,>=2.4.0 (from chromadb)\n",
            "  Using cached posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Using cached onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.76.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Using cached bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.20.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Using cached kubernetes-34.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.3)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.4)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from tavily-python) (2.32.5)\n",
            "Requirement already satisfied: tiktoken>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tavily-python) (0.12.0)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (8.3.0)\n",
            "Requirement already satisfied: primp>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (0.15.0)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from duckduckgo-search) (5.4.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-text-splitters) (1.0.4)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.28.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.4.42)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.38.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Using cached opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: opentelemetry-proto==1.38.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.38.0)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Using cached opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Using cached opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Using cached opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog<6.0.0,>=2.4.0->chromadb)\n",
            "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->tavily-python) (3.4.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.5.1->tavily-python) (2024.11.6)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.36.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.2.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Using cached watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.2.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (3.0.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-text-splitters) (0.25.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Using cached chromadb-1.3.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.8 MB)\n",
            "Using cached tavily_python-0.7.12-py3-none-any.whl (15 kB)\n",
            "Using cached duckduckgo_search-8.1.1-py3-none-any.whl (18 kB)\n",
            "Using cached bcrypt-5.0.0-cp39-abi3-manylinux_2_34_x86_64.whl (278 kB)\n",
            "Using cached kubernetes-34.1.0-py2.py3-none-any.whl (2.0 MB)\n",
            "Using cached onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
            "Using cached opentelemetry_exporter_otlp_proto_grpc-1.38.0-py3-none-any.whl (19 kB)\n",
            "Using cached opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl (18 kB)\n",
            "Using cached opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
            "Using cached opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
            "Using cached opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
            "Using cached posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Using cached watchfiles-1.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (456 kB)\n",
            "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Installing collected packages: duckduckgo-search, coloredlogs, bcrypt, backoff, watchfiles, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, tavily-python, opentelemetry-semantic-conventions, kubernetes, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, chromadb\n",
            "  Attempting uninstall: opentelemetry-exporter-otlp-proto-common\n",
            "    Found existing installation: opentelemetry-exporter-otlp-proto-common 1.37.0\n",
            "    Uninstalling opentelemetry-exporter-otlp-proto-common-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-exporter-otlp-proto-common-1.37.0\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.37.0\n",
            "    Uninstalling opentelemetry-api-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.37.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.58b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.58b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.58b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.37.0\n",
            "    Uninstalling opentelemetry-sdk-1.37.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.37.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-5.0.0 chromadb-1.3.4 coloredlogs-15.0.1 duckduckgo-search-8.1.1 kubernetes-34.1.0 onnxruntime-1.23.2 opentelemetry-api-1.38.0 opentelemetry-exporter-otlp-proto-common-1.38.0 opentelemetry-exporter-otlp-proto-grpc-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 posthog-5.4.0 tavily-python-0.7.12 watchfiles-1.1.1\n"
          ]
        }
      ],
      "source": [
        "# Instalaci√≥n (Colab/Local)\n",
        "# Instalaci√≥n (Colab/Local)\n",
        "!pip install -U \"langchain>=0.2.11\" \"langchain-core>=0.2.33\" \"langchain-community>=0.2.11\" \"langchain-openai>=0.2.1\" \"langsmith>=0.1.97\"\n",
        "# Opcionales para el assignment:\n",
        "!pip install -U faiss-cpu chromadb tavily-python duckduckgo-search langchain-text-splitters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDi67zAYBhtn",
        "outputId": "5907e6ee-ea1a-40de-d347-e685e1d64109"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Un Transformer es un modelo de arquitectura de redes neuronales basado en mecanismos de atenci√≥n que permite procesar secuencias de datos de manera eficiente y paralela, siendo ampliamente utilizado en tareas de procesamiento de lenguaje natural.\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.1)  # modelo sugerido\n",
        "\n",
        "# Hola LLM\n",
        "resp = llm.invoke(\"Defin√≠ 'Transformer' en una sola oraci√≥n.\")\n",
        "print(resp.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF2iLN1GCFBh",
        "outputId": "3a7ec573-71eb-4c89-bc46-055d3e8a6c71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¬°Hola! Soy ChatGPT, un modelo de lenguaje desarrollado por OpenAI basado en la arquitectura GPT-4.\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Completar par√°metros b√°sicos (ver [1] y [12])\n",
        "MODEL = \"gpt-4.1-mini\"         # p.ej., \"gpt-5-mini\"\n",
        "TEMP = 0            # 0.0‚Äì1.0 (determinismo vs creatividad)# Primera prueba, modelo mas determinismo\n",
        "\n",
        "llm = ChatOpenAI(model=MODEL, temperature=TEMP)\n",
        "print(llm.invoke(\"Hola! Decime tu versi√≥n en una l√≠nea.\").content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6t2akKXCSVn",
        "outputId": "a29b6169-2460-4eaa-fbc4-663f8cc43f8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¬°Hola! Soy ChatGPT, un modelo de lenguaje desarrollado por OpenAI basado en la arquitectura GPT-4.\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Completar par√°metros b√°sicos (ver [1] y [12])\n",
        "MODEL = \"gpt-4.1-mini\"         # p.ej., \"gpt-5-mini\"\n",
        "TEMP = 0.5           # 0.0‚Äì1.0 (determinismo vs creatividad)# segunda prueba, 50% mas creativo\n",
        "\n",
        "llm = ChatOpenAI(model=MODEL, temperature=TEMP)\n",
        "print(llm.invoke(\"Hola! Decime tu versi√≥n en una l√≠nea.\").content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bD3lpOxCfN2",
        "outputId": "36b1559c-f48b-482e-c5fe-c5eeed1b944f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¬°Hola! Soy ChatGPT, un modelo de lenguaje desarrollado por OpenAI basado en la arquitectura GPT-4.\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Completar par√°metros b√°sicos (ver [1] y [12])\n",
        "MODEL = \"gpt-4.1-mini\"         # p.ej., \"gpt-5-mini\"\n",
        "TEMP = 1            # 0.0‚Äì1.0 (determinismo vs creatividad)# tercera prueba, modelo super creativo\n",
        "\n",
        "llm = ChatOpenAI(model=MODEL, temperature=TEMP)\n",
        "print(llm.invoke(\"Hola! Decime tu versi√≥n en una l√≠nea.\").content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZPvIAFpJp9r",
        "outputId": "097d2646-c224-43e7-947f-c857251099f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Temperature=0.0 ---\n",
            "[1] ¬°Felicidades al equipo por su incre√≠ble paper de IA! Avanzamos hacia un futuro m√°s inteligente y sorprendente. üöÄü§ñ #IA\n",
            "[2] - Manejan secuencias completas en paralelo, mejorando la eficiencia en el entrenamiento.  \n",
            "- Capturan dependencias a largo plazo gracias al mecanismo de atenci√≥n.  \n",
            "- Son altamente escalables y adaptables a diversas tareas de procesamiento de lenguaje natural.\n",
            "\n",
            "--- Temperature=0.5 ---\n",
            "[1] ¬°Felicidades al equipo por su innovador paper de IA! Un gran avance que impulsa el futuro tecnol√≥gico. #IA #Innovaci√≥n\n",
            "[2] - Manejan dependencias a largo plazo eficientemente gracias al mecanismo de atenci√≥n auto-regresiva.  \n",
            "- Permiten procesamiento paralelo, acelerando el entrenamiento en comparaci√≥n con modelos secuenciales.  \n",
            "- Son altamente escalables y adaptables a diversas tareas de NLP y visi√≥n por computadora.\n",
            "\n",
            "--- Temperature=0.9 ---\n",
            "[1] ¬°Impresionante avance en IA! Este paper abre nuevas puertas para el futuro de la inteligencia artificial. #Innovaci√≥nAI\n",
            "[2] - Manejan secuencias completas en paralelo, mejorando la eficiencia comparado con modelos secuenciales.\n",
            "- Capturan dependencias a largo plazo mediante mecanismos de atenci√≥n, sin importar la distancia entre palabras.\n",
            "- Son altamente escalables y adaptables a diversas tareas de NLP y visi√≥n gracias a su arquitectura modular.\n"
          ]
        }
      ],
      "source": [
        "prompts = [\n",
        "    \"Escrib√≠ un tuit (<=20 palabras) celebrando un paper de IA.\",\n",
        "    \"Dame 3 bullets concisos sobre ventajas de los Transformers.\"\n",
        "]\n",
        "\n",
        "for t in [0.0, 0.5, 0.9]:\n",
        "    llm_t = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=t)\n",
        "    outs = [llm_t.invoke(p).content for p in prompts]\n",
        "    print(f\"\\n--- Temperature={t} ---\")\n",
        "    for i, o in enumerate(outs, 1):\n",
        "        print(f\"[{i}] {o}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRQDhLp4J5E6",
        "outputId": "b3f3f672-6a97-42d9-e7a6-f263dcc016bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datos que hablan,  \n",
            "modelos se ponen a prueba,  \n",
            "veredicto fiel.\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "MODEL = \"gpt-4.1-mini\"\n",
        "TEMP = 0.6      # 0.0, 0.5, 0.9\n",
        "top = 0.9\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(model=MODEL, temperature=TEMP,top_p= top)\n",
        "print(llm.invoke(\"Escrib√≠ un haiku sobre evaluaci√≥n de modelos.\").content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAiRsAYO02Fj",
        "outputId": "a79b4c55-fc35-48a1-c9e6-a0fe767bb1e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La atenci√≥n multi-cabeza es un mecanismo en redes neuronales que permite al modelo enfocarse simult√°neamente en diferentes partes de la entrada, capturando m√∫ltiples relaciones contextuales. Por ejemplo, en traducci√≥n autom√°tica, una cabeza puede atender a la estructura gramatical mientras otra se centra en el significado sem√°ntico, mejorando la precisi√≥n. Esto se logra dividiendo la representaci√≥n en subespacios y aplicando atenci√≥n paralela, cuyos resultados se combinan para una comprensi√≥n m√°s rica.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# 1. El Template\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Sos un asistente conciso, exacto y profesional.\"),\n",
        "    (\"human\",  \"Explic√° {tema} en <= 3 oraciones, con un ejemplo real.\")\n",
        "])\n",
        "\n",
        "# 2. La Cadena (LCEL)\n",
        "#    Nota: 'llm' ya lo definimos en la parte anterior (con T=0)\n",
        "chain = prompt | llm  # LCEL: Encadena el prompt con el modelo\n",
        "\n",
        "# 3. La Invocaci√≥n\n",
        "print(chain.invoke({\"tema\": \"atenci√≥n multi-cabeza\"}).content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbDU3lYB0-BH",
        "outputId": "dc7806c2-38ac-4ce1-e7a0-54df059ae6bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La atenci√≥n multi-cabeza es un mecanismo que permite a un modelo de lenguaje enfocarse simult√°neamente en diferentes partes de una oraci√≥n para captar m√∫ltiples relaciones contextuales. Cada \"cabeza\" procesa la informaci√≥n de manera independiente y luego se combinan para mejorar la comprensi√≥n global. Por ejemplo, en traducci√≥n autom√°tica, una cabeza puede centrarse en la estructura gramatical mientras otra atiende al significado sem√°ntico.\n"
          ]
        }
      ],
      "source": [
        "# Template modificado con un ejemplo (few-shot)\n",
        "prompt_few_shot = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Sos un asistente conciso, exacto y profesional.\"),\n",
        "\n",
        "    # --- Ejemplo (Few-Shot) ---\n",
        "    (\"human\", \"Explic√° {tema} en <= 3 oraciones, con un ejemplo real.\"), # Este input es un placeholder para el ejemplo\n",
        "    (\"ai\", \"Tema: RAG\\nRespuesta: RAG (Generaci√≥n Aumentada por Recuperaci√≥n) evita alucinaciones...\\nEjemplo: Un chatbot de soporte que consulta un manual PDF.\"),\n",
        "    # --------------------------\n",
        "\n",
        "    (\"human\",  \"Explic√° {tema} en <= 3 oraciones, con un ejemplo real.\") # El input real\n",
        "])\n",
        "\n",
        "chain_fs = prompt_few_shot | llm\n",
        "print(chain_fs.invoke({\"tema\": \"atenci√≥n multi-cabeza\"}).content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWaRmjpA1ZE7",
        "outputId": "d8625538-d271-4ca2-dfaf-4d1390d1d5fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class '__main__.Resumen'>\n",
            "title=\"Riesgos de la 'prompt injection' en aplicaciones con LLM\" bullets=['Manipulaci√≥n del modelo para generar respuestas no deseadas o maliciosas, comprometiendo la integridad del sistema.', 'Filtraci√≥n o exposici√≥n inadvertida de informaci√≥n sensible debido a instrucciones inyectadas en el prompt.', 'P√©rdida de confianza del usuario y posibles da√±os legales o reputacionales derivados de comportamientos inesperados del modelo.']\n"
          ]
        }
      ],
      "source": [
        "from typing import List\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# 1. Definimos el \"esquema\" o \"contrato\" de salida.\n",
        "#    Queremos un t√≠tulo y una lista de vi√±etas (bullets).\n",
        "class Resumen(BaseModel):\n",
        "    title: str\n",
        "    bullets: List[str]\n",
        "\n",
        "# 2. Creamos un LLM que *entiende* este esquema.\n",
        "#    Usamos .with_structured_output() para vincular el LLM a la clase Resumen.\n",
        "#    'llm' es el que ya ten√≠as definido (T=0)\n",
        "llm_json = llm.with_structured_output(Resumen)  # ¬°Aqu√≠ ocurre la magia!\n",
        "\n",
        "# 3. Invocamos\n",
        "pedido = \"Resum√≠ en 3 bullets los riesgos de la 'prompt injection' en LLM apps.\"\n",
        "res = llm_json.invoke(pedido)\n",
        "\n",
        "# 4. Verificamos el resultado\n",
        "print(type(res)) # Ver√°s que 'res' no es un string, es un objeto Resumen\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NisUIid19F9",
        "outputId": "7f05c911-4c3e-4a52-a259-36ff62d2bde7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traza enviada (ver LangSmith).\n"
          ]
        }
      ],
      "source": [
        "_ = (prompt | llm).invoke({\"tema\": \"transformers vs RNNs\"})\n",
        "print(\"Traza enviada (ver LangSmith).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCbqT7zk4RQi",
        "outputId": "856dac95-b372-4ff5-b372-41e791a34c6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text='Excelente trabalho da equipe' lang='pt'\n",
            "content='La ventaja de structured output es que permite generar respuestas en un formato JSON estructurado, facilitando la interpretaci√≥n y procesamiento autom√°tico de la informaci√≥n.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 55, 'total_tokens': 83, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'id': 'chatcmpl-CbCnZiLWsH8DMIUs1vSSXOyxinMve', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--86e2f63e-ff16-4177-82ff-bae24095bdcc-0' usage_metadata={'input_tokens': 55, 'output_tokens': 28, 'total_tokens': 83, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "# Esqueleto sugerido para 1) y 2)\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class Traduccion(BaseModel):\n",
        "    text: str\n",
        "    lang: str\n",
        "\n",
        "traductor = llm.with_structured_output(Traduccion)\n",
        "salida = traductor.invoke(\"Traduc√≠ al portugu√©s: 'Excelente trabajo del equipo'.\")\n",
        "print(salida)\n",
        "\n",
        "# Q&A con contexto (sin RAG)\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "QA_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Respond√© SOLO usando el contexto. Si no alcanza, dec√≠ 'No suficiente contexto'.\"),\n",
        "    (\"human\",  \"Contexto:\\n{contexto}\\n\\nPregunta: {pregunta}\\nRespuesta breve:\")\n",
        "])\n",
        "salida = (QA_prompt | llm).invoke({\n",
        "    \"contexto\": \"OpenAI y LangChain permiten structured output con JSON...\",\n",
        "    \"pregunta\": \"¬øQu√© ventaja tiene structured output?\"\n",
        "})\n",
        "print(salida)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmFK8JGL5EiG",
        "outputId": "9690ddec-f603-48d2-e8ce-2fdb68d0a866"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== Zero-shot ==\n",
            "Texto: 'Me encant√≥ la experiencia, repetir√≠a.' -> POS\n",
            "Texto: 'No cumple lo prometido; decepcionante.' -> NEG\n",
            "Texto: 'Est√° bien, nada extraordinario.' -> NEU\n",
            "\n",
            "== Few-shot ==\n",
            "Texto: 'Me encant√≥ la experiencia, repetir√≠a.' -> POS\n",
            "Texto: 'No cumple lo prometido; decepcionante.' -> NEG\n",
            "Texto: 'Est√° bien, nada extraordinario.' -> POS\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model=\"gpt-5-mini\", temperature=0.5)\n",
        "\n",
        "# --- Plantilla 1: Zero-shot ---\n",
        "zs_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Sos un asistente conciso y exacto.\"),\n",
        "    (\"human\",  \"Clasific√° el sentimiento de este texto como POS, NEG o NEU:\\n\\n{texto}\")\n",
        "])\n",
        "\n",
        "# --- Plantilla 2: Few-shot (con ejemplos) ---\n",
        "fs_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Sos un asistente conciso y exacto.\"),\n",
        "    # Ejemplos para guiar el formato\n",
        "    (\"human\",  \"Ejemplo:\\nTexto: 'El producto super√≥ mis expectativas'\\nEtiqueta: POS\"),\n",
        "    (\"ai\", \"POS\"), # Respuesta de IA al ejemplo\n",
        "    (\"human\",  \"Ejemplo:\\nTexto: 'La entrega fue tarde y vino roto'\\nEtiqueta: NEG\"),\n",
        "    (\"ai\", \"NEG\"), # Respuesta de IA al ejemplo\n",
        "\n",
        "    # Tarea real\n",
        "    (\"human\",  \"Texto: {texto}\\nEtiqueta:\") # El modelo debe completar esto\n",
        "])\n",
        "\n",
        "textos = [\n",
        "    \"Me encant√≥ la experiencia, repetir√≠a.\",\n",
        "    \"No cumple lo prometido; decepcionante.\",\n",
        "    \"Est√° bien, nada extraordinario.\" # Este es el m√°s dif√≠cil\n",
        "]\n",
        "\n",
        "print(\"== Zero-shot ==\")\n",
        "for t in textos:\n",
        "    # Creamos la cadena y la invocamos\n",
        "    chain_zs = zs_prompt | llm\n",
        "    print(f\"Texto: '{t}' -> {chain_zs.invoke({'texto': t}).content}\")\n",
        "\n",
        "print(\"\\n== Few-shot ==\")\n",
        "for t in textos:\n",
        "    # Creamos la cadena y la invocamos\n",
        "    chain_fs = fs_prompt | llm\n",
        "    print(f\"Texto: '{t}' -> {chain_fs.invoke({'texto': t}).content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhKOBsmG_UMe",
        "outputId": "bdebc6af-8675-4bb3-9218-7c0fe6631891"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Texto dividido en 2 chunks.\n",
            "\n",
            "--- RES√öMENES INTERMEDIOS (MAP) ---\n",
            "- Los Transformers introdujeron la auto-atenci√≥n, permitiendo el procesamiento paralelo de secuencias en NLP, a diferencia de las RNNs que procesan secuencialmente.  \n",
            "- Esta arquitectura facilita la captura efectiva de dependencias a larga distancia en el texto.  \n",
            "- Modelos destacados como BERT y GPT est√°n basados en la arquitectura de Transformers.\n",
            "- Los Transformers introdujeron la auto-atenci√≥n, permitiendo el procesamiento paralelo de secuencias en NLP, a diferencia de las RNNs que procesan secuencialmente.  \n",
            "- Esta arquitectura facilita la captura efectiva de dependencias a larga distancia en el texto.  \n",
            "- Modelos destacados como BERT y GPT est√°n basados en la arquitectura de Transformers.\n",
            "\n",
            "--- RESUMEN FINAL (REDUCE) ---\n",
            "Los Transformers revolucionaron el procesamiento de lenguaje natural al introducir la auto-atenci√≥n, que permite el procesamiento paralelo de secuencias, a diferencia de las RNNs que lo hacen secuencialmente. Esta arquitectura facilita la captura efectiva de dependencias a larga distancia en el texto. Modelos destacados como BERT y GPT est√°n basados en Transformers, aprovechando sus ventajas para mejorar tareas de NLP.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0) # Volvemos a T=0 para res√∫menes factuales\n",
        "\n",
        "# --- Texto largo de ejemplo ---\n",
        "# (Pega aqu√≠ un texto largo. Para probar, simplemente copia y pega este bloque 3 o 4 veces)\n",
        "long_text = \"\"\"\n",
        "Los Transformers revolucionaron el NLP al introducir la auto-atenci√≥n,\n",
        "permitiendo el procesamiento paralelo de secuencias, a diferencia de las RNNs\n",
        "que procesan de forma secuencial. Esta arquitectura permite capturar\n",
        "dependencias a larga distancia de manera m√°s efectiva.\n",
        "Modelos como BERT y GPT se basan en esta arquitectura fundamental.\n",
        "\"\"\"\n",
        "long_text = long_text * 4 # Simulamos un texto 4 veces m√°s largo\n",
        "\n",
        "# 1. Split (Dividir)\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=100)\n",
        "chunks = splitter.split_text(long_text)\n",
        "print(f\"Texto dividido en {len(chunks)} chunks.\")\n",
        "\n",
        "# 2. Map (Cadena para resumir un chunk)\n",
        "chunk_summary_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Resum√≠ el siguiente fragmento en 2‚Äì3 bullets, claros y factuales.\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "chunk_summary_chain = chunk_summary_prompt | llm\n",
        "\n",
        "# Obtenemos los res√∫menes de cada chunk\n",
        "bullets_list = [chunk_summary_chain.invoke({\"input\": c}).content for c in chunks]\n",
        "bullets_combined = \"\\n\".join(bullets_list)\n",
        "print(\"\\n--- RES√öMENES INTERMEDIOS (MAP) ---\")\n",
        "print(bullets_combined)\n",
        "\n",
        "\n",
        "# 3. Reduce (Combinar resultados)\n",
        "reduce_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Consolid√° estos bullets (pueden ser redundantes) y produc√≠ un resumen √∫nico y coherente.\"),\n",
        "    (\"human\", \"Bullets:\\n{bullets}\\n\\nResumen final (<=120 tokens):\")\n",
        "])\n",
        "final_chain = (reduce_prompt | llm)\n",
        "\n",
        "final_summary = final_chain.invoke({\"bullets\": bullets_combined}).content\n",
        "print(\"\\n--- RESUMEN FINAL (REDUCE) ---\")\n",
        "print(final_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiHfbzezAxKp",
        "outputId": "4b4c3629-0df5-4eeb-87ce-eb91860ea00f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "titulo='OpenAI anunci√≥ una colaboraci√≥n con la Universidad Catolica del Uruguay' fecha='05/11/2025' entidades=[Entidad(tipo='ORG', valor='OpenAI'), Entidad(tipo='ORG', valor='Universidad Catolica del Uruguay'), Entidad(tipo='LOC', valor='Montevideo')]\n"
          ]
        }
      ],
      "source": [
        "from typing import List, Optional\n",
        "from pydantic import BaseModel\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
        "# 1. Definimos el esquema de extracci√≥n\n",
        "class Entidad(BaseModel):\n",
        "    tipo: str   # p.ej., 'ORG', 'PER', 'LOC'\n",
        "    valor: str\n",
        "\n",
        "class ExtractInfo(BaseModel):\n",
        "    titulo: Optional[str] # 'Optional' significa que puede ser None\n",
        "    fecha: Optional[str]\n",
        "    entidades: List[Entidad]\n",
        "\n",
        "# 2. Creamos el extractor\n",
        "extractor = llm.with_structured_output(ExtractInfo)\n",
        "\n",
        "# 3. Invocamos\n",
        "texto = \"OpenAI anunci√≥ una colaboraci√≥n con la Universidad Catolica del Uruguay en Montevideo el 05/11/2025.\"\n",
        "resultado_extraccion = extractor.invoke(f\"Extra√© titulo, fecha y entidades (ORG/PER/LOC) del siguiente texto:\\n\\n{texto}\")\n",
        "\n",
        "print(resultado_extraccion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EFcmq5aCOUj",
        "outputId": "e4b00212-5837-413f-88f1-356d249a69cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input': 'C√≥mo se usa Pydantic?', 'context': [Document(id='35d2cfbf-394a-4477-b1aa-67db9d2abf9e', metadata={}, page_content='LangChain soporta structured output con Pydantic.'), Document(id='60c2bdad-8387-458c-993e-ad2b56e36fcc', metadata={}, page_content='OpenAIEmbeddings facilita embeddings para indexar textos.')], 'answer': 'LangChain soporta structured output con Pydantic, lo que permite definir modelos de datos estructurados para la salida de los procesos. Para usar Pydantic, se debe crear una clase que herede de BaseModel de Pydantic, definiendo los campos y sus tipos. Luego, LangChain puede utilizar esta clase para validar y estructurar la salida generada.'}\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS  # Base de datos vectorial en memoria\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_classic.chains.combine_documents.stuff import create_stuff_documents_chain\n",
        "from langchain_classic.chains import create_retrieval_chain\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# --- 1. INDEXAR (Setup) ---\n",
        "\n",
        "# Documentos de ejemplo (tu base de conocimiento)\n",
        "docs_raw = [\n",
        "    \"LangChain soporta structured output con Pydantic.\",\n",
        "    \"RAG combina recuperaci√≥n + generaci√≥n para mejor grounding.\",\n",
        "    \"OpenAIEmbeddings facilita embeddings para indexar textos.\"]\n",
        "docs = [Document(page_content=t) for t in docs_raw]\n",
        "\n",
        "# Split (Dividir)\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "# Embed (Vectorizar) y Store (Almacenar)\n",
        "emb = OpenAIEmbeddings()\n",
        "vs = FAISS.from_documents(chunks, embedding=emb)\n",
        "\n",
        "# --- 2. RECUPERAR y GENERAR (Runtime) ---\n",
        "\n",
        "# 'llm' ya lo tenemos definido (T=0)\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
        "\n",
        "# El \"Retriever\" es el motor de b√∫squeda\n",
        "retriever = vs.as_retriever(search_kwargs={\"k\": 2}) # k=2 -> dame los 2 mejores chunks\n",
        "\n",
        "# El prompt (igual que en la Parte 5)\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Respond√© SOLO con el contexto. Si no alcanza, dec√≠ 'No suficiente contexto'.\"),\n",
        "    (\"human\",  \"Contexto:\\n{context}\\n\\nPregunta: {input}\")\n",
        "])\n",
        "\n",
        "# Cadena para \"rellenar\" el prompt con los documentos\n",
        "combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "# Cadena RAG completa:\n",
        "# 1. Llama al 'retriever' con el 'input'\n",
        "# 2. El 'retriever' devuelve los 'documentos'\n",
        "# 3. Los 'documentos' (context) y el 'input' (pregunta) se pasan a 'combine_docs_chain'\n",
        "rag_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
        "\n",
        "# Invocamos la cadena RAG completa\n",
        "resultado_rag = rag_chain.invoke({\"input\": \"C√≥mo se usa Pydantic?\"})\n",
        "\n",
        "print(resultado_rag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzkpPUV1CkQe",
        "outputId": "015070b3-e08a-4cfd-f393-fc605bb4a27e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Chatbot de Soporte listo ---\n"
          ]
        }
      ],
      "source": [
        "from typing import List, Literal\n",
        "from pydantic import BaseModel\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "# --- 1. Definir el Esquema de Salida (Como en Parte 3 y 8) ---\n",
        "#    Este es el JSON que debe devolver\n",
        "class Fuente(BaseModel):\n",
        "    title: str\n",
        "    url: str # Para un RAG local, la URL puede ser \"Manual-Producto.pdf\"\n",
        "\n",
        "class RespuestaSoporte(BaseModel):\n",
        "    answer: str\n",
        "    sources: List[Fuente]\n",
        "    confidence: Literal[\"low\", \"medium\", \"high\"] # 'Literal' fuerza una de estas 3 opciones\n",
        "\n",
        "# --- 2. Setup del LLM y el Corpus (FAQs) ---\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
        "\n",
        "# El LLM que sabe c√≥mo generar el JSON\n",
        "llm_structured = llm.with_structured_output(RespuestaSoporte)\n",
        "\n",
        "# Corpus de FAQs (Tu base de conocimiento)\n",
        "docs_raw = [\n",
        "    Document(page_content=\"Para reiniciar el producto X, mantenga presionado el bot√≥n rojo por 5 segundos.\", metadata={\"title\": \"Gu√≠a R√°pida\", \"url\": \"manual.pdf\"}),\n",
        "    Document(page_content=\"La garant√≠a del producto X cubre defectos de f√°brica por 1 a√±o.\", metadata={\"title\": \"T√©rminos de Garant√≠a\", \"url\": \"garantia.pdf\"}),\n",
        "    Document(page_content=\"La luz azul parpadeante indica que el dispositivo est√° buscando conexi√≥n WiFi.\", metadata={\"title\": \"Gu√≠a de Luces\", \"url\": \"manual.pdf\"})\n",
        "]\n",
        "\n",
        "# --- 3. Setup del RAG (Como en Parte 9) ---\n",
        "emb = OpenAIEmbeddings()\n",
        "vs = FAISS.from_documents(docs_raw, embedding=emb)\n",
        "retriever = vs.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# --- 4. El Prompt Avanzado ---\n",
        "#    Instruye al modelo para que llene el JSON\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"\n",
        "     Eres un asistente de soporte t√©cnico para el 'Producto X'. Responde la pregunta del usuario\n",
        "     bas√°ndote √öNICAMENTE en el contexto de las FAQs proporcionado.\n",
        "\n",
        "     Si el contexto es suficiente, responde y cita la fuente (title y url).\n",
        "     Si el contexto no es suficiente, di 'No encontr√© informaci√≥n sobre eso.'\n",
        "\n",
        "     Genera SIEMPRE el objeto JSON 'RespuestaSoporte' completo.\n",
        "     Determina la confianza (confidence) seg√∫n cu√°n bien el contexto responde la pregunta.\n",
        "     \"\"\"),\n",
        "    (\"human\",  \"Contexto de FAQs:\\n{context}\\n\\nPregunta: {question}\")\n",
        "])\n",
        "\n",
        "# --- 5. La Cadena RAG Completa (LCEL) ---\n",
        "\n",
        "# Funci√≥n para formatear los documentos recuperados\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(f\"Fuente: {doc.metadata['title']}\\nContenido: {doc.page_content}\" for doc in docs)\n",
        "\n",
        "# La cadena LCEL\n",
        "# RunnablePassthrough() pasa la pregunta del usuario\n",
        "# RunnableParallel() ejecuta la recuperaci√≥n y pasa la pregunta al mismo tiempo\n",
        "rag_chain_structured = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm_structured\n",
        ")\n",
        "\n",
        "print(\"--- Chatbot de Soporte listo ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf_J51NOC2N6",
        "outputId": "bab1050f-933d-482b-89f4-1e6263b6613c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Pregunta 1 (Respuesta esperada: S√ç) ---\n",
            "answer='La luz azul parpadeante indica que el dispositivo est√° buscando conexi√≥n WiFi.' sources=[Fuente(title='Gu√≠a de Luces', url='#')] confidence='high'\n",
            "\n",
            "--- Pregunta 2 (Respuesta esperada: NO) ---\n",
            "answer='No encontr√© informaci√≥n sobre c√≥mo cambiar la bater√≠a en el contexto proporcionado.' sources=[Fuente(title='Gu√≠a R√°pida', url='N/A'), Fuente(title='Gu√≠a de Luces', url='N/A')] confidence='low'\n"
          ]
        }
      ],
      "source": [
        "# --- Prueba 1: Pregunta con respuesta en el corpus ---\n",
        "print(\"--- Pregunta 1 (Respuesta esperada: S√ç) ---\")\n",
        "resultado_1 = rag_chain_structured.invoke(\"¬øQu√© significa la luz azul parpadeante?\")\n",
        "print(resultado_1)\n",
        "\n",
        "# --- Prueba 2: Pregunta sin respuesta en el corpus ---\n",
        "print(\"\\n--- Pregunta 2 (Respuesta esperada: NO) ---\")\n",
        "resultado_2 = rag_chain_structured.invoke(\"¬øC√≥mo cambio la bater√≠a?\")\n",
        "print(resultado_2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
