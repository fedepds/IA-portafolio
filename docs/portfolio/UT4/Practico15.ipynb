{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXJx-qZ5CmgJ",
        "outputId": "cdc296d8-e722-44af-f6fd-9275b88d5bc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langgraph>=0.2.0\n",
            "  Downloading langgraph-1.0.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: langchain>=0.2.11 in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Collecting langchain>=0.2.11\n",
            "  Downloading langchain-1.1.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: langchain-core>=0.2.33 in /usr/local/lib/python3.12/dist-packages (0.3.80)\n",
            "Collecting langchain-core>=0.2.33\n",
            "  Downloading langchain_core-1.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting langchain-community>=0.2.11\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-openai>=0.2.1\n",
            "  Downloading langchain_openai-1.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (0.3.11)\n",
            "Collecting langchain-text-splitters\n",
            "  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph>=0.2.0)\n",
            "  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph>=0.2.0)\n",
            "  Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph>=0.2.0)\n",
            "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph>=0.2.0) (2.11.10)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph>=0.2.0) (3.6.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.2.33) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.2.33) (0.4.43)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.2.33) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.2.33) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.2.33) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.2.33) (4.15.0)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community>=0.2.11)\n",
            "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.2.11) (2.0.44)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain-community>=0.2.11)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.2.11) (3.13.2)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community>=0.2.11)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.2.11) (2.12.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.2.11) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community>=0.2.11) (2.0.2)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai>=0.2.1) (1.109.1)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai>=0.2.1) (0.12.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.11) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.11) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.11) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.11) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.11) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.11) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.11) (1.22.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community>=0.2.11)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community>=0.2.11)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.2.33) (3.0.0)\n",
            "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph>=0.2.0)\n",
            "  Downloading ormsgpack-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph>=0.2.0) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph>=0.2.0) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.2.33) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.2.33) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai>=0.2.1) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai>=0.2.1) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai>=0.2.1) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai>=0.2.1) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai>=0.2.1) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph>=0.2.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph>=0.2.0) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph>=0.2.0) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community>=0.2.11) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community>=0.2.11) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community>=0.2.11) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community>=0.2.11) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community>=0.2.11) (2025.11.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community>=0.2.11) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai>=0.2.1) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph>=0.2.0) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph>=0.2.0) (0.16.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community>=0.2.11)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langgraph-1.0.3-py3-none-any.whl (156 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-1.1.0-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.1.0-py3-none-any.whl (473 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m473.8/473.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-1.1.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.3/84.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.13.0-cp39-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-1.0.5-py3-none-any.whl (35 kB)\n",
            "Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.12.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (208 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, ormsgpack, mypy-extensions, marshmallow, faiss-cpu, typing-inspect, langgraph-sdk, dataclasses-json, langchain-core, langgraph-checkpoint, langchain-text-splitters, langchain-openai, langgraph-prebuilt, langchain-classic, langgraph, langchain-community, langchain\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.80\n",
            "    Uninstalling langchain-core-0.3.80:\n",
            "      Successfully uninstalled langchain-core-0.3.80\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.11\n",
            "    Uninstalling langchain-text-splitters-0.3.11:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.11\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.27\n",
            "    Uninstalling langchain-0.3.27:\n",
            "      Successfully uninstalled langchain-0.3.27\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 faiss-cpu-1.13.0 langchain-1.1.0 langchain-classic-1.0.0 langchain-community-0.4.1 langchain-core-1.1.0 langchain-openai-1.1.0 langchain-text-splitters-1.0.0 langgraph-1.0.3 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.5 langgraph-sdk-0.2.9 marshmallow-3.26.1 mypy-extensions-1.1.0 ormsgpack-1.12.0 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U \"langgraph>=0.2.0\" \\\n",
        "               \"langchain>=0.2.11\" \"langchain-core>=0.2.33\" \\\n",
        "               \"langchain-community>=0.2.11\" \"langchain-openai>=0.2.1\" \\\n",
        "               \"faiss-cpu\" \"langchain-text-splitters\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLiBdyWeDBH3",
        "outputId": "6e4b3c11-2f1e-41d6-a41c-f23be9320b1f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pydantic/v1/main.py:1054: UserWarning: LangSmith now uses UUID v7 for run and trace identifiers. This warning appears when passing custom IDs. Please use: from langsmith import uuid7\n",
            "            id = uuid7()\n",
            "Future versions will require UUID v7.\n",
            "  input_data = validator(cls_, input_data)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¬°Eso suena emocionante! LangGraph es una herramienta poderosa para trabajar con modelos de lenguaje y crear agentes conversacionales. Si tienes alguna pregunta espec√≠fica o necesitas ayuda con algo relacionado con tu agente, no dudes en preguntar. Estoy aqu√≠ para ayudarte. ¬øQu√© est√°s tratando de lograr con tu agente LangGraph?\n"
          ]
        }
      ],
      "source": [
        "from typing_extensions import TypedDict, Annotated\n",
        "import operator\n",
        "from langgraph.graph import StateGraph, START, END   # ‚Üê importar START y END\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list, operator.add]\n",
        "\n",
        "# Modelo del agente\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "def assistant_node(state: AgentState) -> AgentState:\n",
        "    # Llamar al modelo usando TODO el historial\n",
        "    response = llm.invoke(state[\"messages\"])\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# Construcci√≥n del grafo\n",
        "builder = StateGraph(AgentState)\n",
        "\n",
        "builder.add_node(\"assistant\", assistant_node)\n",
        "\n",
        "# Transiciones del flujo\n",
        "builder.add_edge(START, \"assistant\")\n",
        "builder.add_edge(\"assistant\", END)\n",
        "\n",
        "# Compilaci√≥n del grafo\n",
        "graph = builder.compile()\n",
        "\n",
        "# Estado inicial\n",
        "initial_state = {\n",
        "    \"messages\": [HumanMessage(content=\"Probando mi primer agente LangGraph :)\")]\n",
        "}\n",
        "\n",
        "# Ejecuci√≥n del agente\n",
        "result = graph.invoke(initial_state)\n",
        "print(result[\"messages\"][-1].content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1QJnztP7ksO"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BVQPiBN7ECzi"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "from typing_extensions import TypedDict, Annotated\n",
        "import operator\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list, operator.add]\n",
        "    summary: Optional[str]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zx6M81vmEJsv"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "from typing_extensions import TypedDict, Annotated\n",
        "import operator\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list, operator.add]\n",
        "    summary: Optional[str]\n",
        "\n",
        "# Estado inicial\n",
        "initial_state = {\n",
        "    \"messages\": [],\n",
        "    \"summary\": None\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GXn7znNRENcR"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# Corpus m√≠nimo (pod√©s cambiarlo por algo de tu dominio)\n",
        "raw_docs = [\n",
        "    \"LangGraph permite orquestar agentes como grafos de estado.\",\n",
        "    \"RAG combina recuperaci√≥n + generaci√≥n para mejorar grounding.\",\n",
        "    \"LangChain y LangGraph se integran con OpenAI, HuggingFace y m√°s.\"\n",
        "]\n",
        "\n",
        "docs = [Document(page_content=t) for t in raw_docs]\n",
        "\n",
        "# Split en chunks\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "chunks = splitter.split_documents(docs)\n",
        "\n",
        "# Vector store FAISS\n",
        "emb = OpenAIEmbeddings()\n",
        "vs = FAISS.from_documents(chunks, embedding=emb)\n",
        "retriever = vs.as_retriever(search_kwargs={\"k\": 3})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7WJhRcwr8YT1"
      },
      "outputs": [],
      "source": [
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def rag_search(question: str) -> str:\n",
        "    \"\"\"\n",
        "    Tool de recuperaci√≥n (RAG).\n",
        "    Recibe una pregunta como string y devuelve texto relevante recuperado\n",
        "    desde el vector store. Ideal para que el LLM use como contexto.\n",
        "    \"\"\"\n",
        "    docs = retriever.vectorstore.similarity_search(\n",
        "        question,\n",
        "        k=retriever.search_kwargs.get(\"k\", 3),\n",
        "    )\n",
        "    context = \"\\n\\n\".join(d.page_content for d in docs)\n",
        "    if not context:\n",
        "        return \"No se encontr√≥ informaci√≥n relevante en el corpus.\"\n",
        "    return context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QXcqJCJgEZiP"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "FAKE_ORDERS = {\n",
        "    \"ABC123\": \"En preparaci√≥n\",\n",
        "    \"XYZ999\": \"Entregado\",\n",
        "}\n",
        "\n",
        "@tool\n",
        "def get_order_status(order_id: str) -> str:\n",
        "    \"\"\"\n",
        "    Devuelve el estado de un pedido ficticio dado su ID.\n",
        "    Esta tool simula un servicio de soporte que consulta pedidos.\n",
        "    \"\"\"\n",
        "    status = FAKE_ORDERS.get(order_id)\n",
        "    if status is None:\n",
        "        return f\"No encontr√© el pedido {order_id}.\"\n",
        "    return f\"Estado actual del pedido {order_id}: {status}\"\n",
        "\n",
        "@tool\n",
        "def get_utc_time(_: str = \"\") -> str:\n",
        "    \"\"\"\n",
        "    Devuelve la hora actual en formato ISO (UTC).\n",
        "    \"\"\"\n",
        "    return datetime.utcnow().isoformat()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deEUftCOEdH7",
        "outputId": "84ea021f-e34a-4e0d-f864-f8f2f33d6d30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Grafo compilado exitosamente.\n"
          ]
        }
      ],
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain_core.messages import AIMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langgraph.graph import StateGraph, START, END # Importaciones necesarias\n",
        "\n",
        "# 1) Lista de tools (Definidas en pasos anteriores)\n",
        "tools = [rag_search, get_order_status, get_utc_time]\n",
        "\n",
        "# 2) LLM con tools\n",
        "# Usamos gpt-4o-mini que es eficiente y soporta function calling\n",
        "llm_with_tools = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0).bind_tools(tools)\n",
        "\n",
        "def assistant_node(state: AgentState) -> AgentState:\n",
        "    \"\"\"\n",
        "    Nodo de reasoning: El cerebro del agente.\n",
        "    Decide si responde directo (texto) o solicita usar una tool (tool_call).\n",
        "    \"\"\"\n",
        "    response = llm_with_tools.invoke(state[\"messages\"])\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# 3) Nodo que ejecuta efectivamente las tools\n",
        "# Este nodo toma el output del assistant, busca la funci√≥n correspondiente y la ejecuta.\n",
        "tool_node = ToolNode(tools)\n",
        "\n",
        "# Router para decidir el flujo\n",
        "def route_from_assistant(state: AgentState) -> str:\n",
        "    last = state[\"messages\"][-1]\n",
        "    # Si el LLM gener√≥ una llamada a herramienta, vamos al nodo \"tools\"\n",
        "    if isinstance(last, AIMessage) and last.tool_calls:\n",
        "        return \"tools\"\n",
        "    # Si no, terminamos el flujo\n",
        "    return END\n",
        "\n",
        "# üéØ Construcci√≥n del Grafo\n",
        "builder = StateGraph(AgentState)\n",
        "\n",
        "# Nodos\n",
        "builder.add_node(\"assistant\", assistant_node)\n",
        "builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "# Aristas\n",
        "builder.add_edge(START, \"assistant\")\n",
        "\n",
        "# Decisi√≥n condicional\n",
        "builder.add_conditional_edges(\n",
        "    \"assistant\",\n",
        "    route_from_assistant,\n",
        "    {\n",
        "        \"tools\": \"tools\",\n",
        "        END: END,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Ciclo de retorno (La clave de LangGraph)\n",
        "builder.add_edge(\"tools\", \"assistant\")\n",
        "\n",
        "# Compilaci√≥n\n",
        "graph = builder.compile()\n",
        "\n",
        "print(\"‚úÖ Grafo compilado exitosamente.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6P5SpBW0xW-",
        "outputId": "86061662-817b-4f4e-cdb9-dfc824810afa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Paso del Agente ---\n",
            "\n",
            "--- Paso del Agente ---\n",
            "LangGraph permite orquestar agentes como grafos de estado.\n",
            "\n",
            "LangChain y LangGraph se integran con OpenAI, HuggingFace y m√°s.\n",
            "\n",
            "RAG combina recuperaci√≥n + generaci√≥n para mejorar grounding.\n",
            "--- Paso del Agente ---\n",
            "El estado actual del pedido **ABC123** es: **En preparaci√≥n**.\n",
            "\n",
            "En cuanto a **LangGraph**, permite orquestar agentes como grafos de estado. LangChain y LangGraph se integran con OpenAI, HuggingFace y m√°s. Adem√°s, RAG combina recuperaci√≥n y generaci√≥n para mejorar el grounding.\n"
          ]
        }
      ],
      "source": [
        "# Prueba\n",
        "consulta = \"Hola, necesito saber el estado del pedido ABC123 y tambi√©n quiero saber qu√© es LangGraph seg√∫n tus documentos.\"\n",
        "\n",
        "for event in graph.stream({\"messages\": [(\"user\", consulta)]}):\n",
        "    for value in event.values():\n",
        "        print(\"--- Paso del Agente ---\")\n",
        "        print(value[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOEB2Vxz9INM",
        "outputId": "18fbc792-3b98-469d-bd2d-33c16d1ba989"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Respuesta 1: LangGraph es una herramienta que permite orquestar agentes como grafos de estado. Se integra con plataformas como LangChain, OpenAI y HuggingFace, y utiliza un enfoque de Recuperaci√≥n y Generaci√≥n (RAG) para mejorar el grounding en las interacciones.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "state = {\n",
        "    \"messages\": [\n",
        "        HumanMessage(content=\"Hola, ¬øqu√© es LangGraph en pocas palabras?\")\n",
        "    ],\n",
        "    \"summary\": None\n",
        "}\n",
        "\n",
        "result = graph.invoke(state)\n",
        "print(\"Respuesta 1:\", result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbSuv2wV9KXt",
        "outputId": "75f1adde-9fb6-462e-b2a7-9efcd24784ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Respuesta 2: RAG, que significa Recuperaci√≥n y Generaci√≥n, es un enfoque que combina la recuperaci√≥n de informaci√≥n con la generaci√≥n de texto para mejorar el grounding en las interacciones. Esto permite que los modelos de lenguaje utilicen informaci√≥n relevante de manera m√°s efectiva al generar respuestas.\n"
          ]
        }
      ],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "state2 = deepcopy(result)\n",
        "state2[\"messages\"].append(HumanMessage(content=\"Us√° tu base de conocimiento y decime qu√© es RAG.\"))\n",
        "\n",
        "result2 = graph.invoke(state2)\n",
        "print(\"Respuesta 2:\", result2[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81bCz_sg9P3B",
        "outputId": "1327f6cb-5a46-48d3-82b1-006a86ddbff7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "√öltimo mensaje: human ‚Üí Us√° tu base de conocimiento y decime qu√© es RAG.\n",
            "√öltimo mensaje: ai ‚Üí \n",
            "√öltimo mensaje: tool ‚Üí RAG combina recuperaci√≥n + generaci√≥n para mejorar grounding.\n",
            "\n",
            "LangGraph permite orquestar agentes como grafos de estado.\n",
            "\n",
            "LangChain y LangGraph se integran con OpenAI, HuggingFace y m√°s.\n",
            "√öltimo mensaje: ai ‚Üí RAG, que significa Recuperaci√≥n y Generaci√≥n, es un enfoque que combina la recuperaci√≥n de informaci√≥n con la generaci√≥n de texto para mejorar el grounding en las interacciones. Esto permite que los modelos de lenguaje utilicen informaci√≥n relevante de fuentes externas para generar respuestas m√°s precisas y contextualmente adecuadas.\n"
          ]
        }
      ],
      "source": [
        "for event in graph.stream(state2, stream_mode=\"values\"):\n",
        "    msgs = event[\"messages\"]\n",
        "    print(\"√öltimo mensaje:\", msgs[-1].type, \"‚Üí\", msgs[-1].content if hasattr(msgs[-1], \"content\") else msgs[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPAwd63u9cg6",
        "outputId": "9b61e619-31ce-4092-e370-79ed535c2674"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Grafo con ciclo de memoria compilado correctamente.\n"
          ]
        }
      ],
      "source": [
        "# --- Importaciones necesarias ---\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain_core.messages import AIMessage, SystemMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# --- 1. Definici√≥n de Nodos y L√≥gica ---\n",
        "\n",
        "# (Aseg√∫rate de tener las tools y AgentState definidos en celdas anteriores)\n",
        "\n",
        "# Nodo Asistente (Cerebro)\n",
        "def assistant_node(state: AgentState) -> AgentState:\n",
        "    # Inyectamos el resumen en el contexto si existe\n",
        "    summary = state.get(\"summary\", \"\")\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    if summary:\n",
        "        # A√±adimos un mensaje de sistema con el resumen al inicio\n",
        "        # (Truco: esto le da contexto sin gastar tokens en mensajes viejos si los hubi√©ramos borrado)\n",
        "        system_message = SystemMessage(content=f\"Resumen de la conversaci√≥n anterior: {summary}\")\n",
        "        messages = [system_message] + messages\n",
        "\n",
        "    response = llm_with_tools.invoke(messages)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# Nodo de Memoria (El nuevo componente)\n",
        "def memory_node(state: AgentState) -> AgentState:\n",
        "    summary_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "    current_summary = state.get(\"summary\", \"\") or \"Nada a√∫n.\"\n",
        "\n",
        "    # Tomamos los √∫ltimos mensajes para actualizar el resumen\n",
        "    recent_messages = state[\"messages\"][-5:]\n",
        "    conversation_str = \"\\n\".join([f\"{m.type}: {m.content}\" for m in recent_messages])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Resumen actual: {current_summary}\n",
        "    Nueva conversaci√≥n: {conversation_str}\n",
        "\n",
        "    Genera un nuevo resumen actualizado conciso (max 3 l√≠neas).\n",
        "    \"\"\"\n",
        "    response = summary_llm.invoke(prompt)\n",
        "    return {\"summary\": response.content}\n",
        "\n",
        "# Nodo de Herramientas (Brazos)\n",
        "tool_node = ToolNode(tools)\n",
        "\n",
        "# Router (Sem√°foro)\n",
        "def route_from_assistant(state: AgentState) -> str:\n",
        "    last = state[\"messages\"][-1]\n",
        "    if isinstance(last, AIMessage) and last.tool_calls:\n",
        "        return \"tools\"\n",
        "    return END\n",
        "\n",
        "# --- 2. Construcci√≥n Limpia del Grafo ---\n",
        "\n",
        "# Reiniciamos el builder desde cero\n",
        "builder = StateGraph(AgentState)\n",
        "\n",
        "# A√±adimos TODOS los nodos\n",
        "builder.add_node(\"assistant\", assistant_node)\n",
        "builder.add_node(\"tools\", tool_node)\n",
        "builder.add_node(\"memory\", memory_node)  # <--- Nuevo nodo\n",
        "\n",
        "# Definimos el flujo (Edges)\n",
        "\n",
        "# Inicio -> Asistente\n",
        "builder.add_edge(START, \"assistant\")\n",
        "\n",
        "# Asistente -> ¬øTools o Fin?\n",
        "builder.add_conditional_edges(\n",
        "    \"assistant\",\n",
        "    route_from_assistant,\n",
        "    {\n",
        "        \"tools\": \"tools\",\n",
        "        END: END\n",
        "    }\n",
        ")\n",
        "\n",
        "# --- EL CAMBIO CLAVE EN EL FLUJO ---\n",
        "# Antes era: tools -> assistant\n",
        "# Ahora es:  tools -> memory -> assistant\n",
        "\n",
        "builder.add_edge(\"tools\", \"memory\")      # Despu√©s de actuar, resumimos\n",
        "builder.add_edge(\"memory\", \"assistant\")  # Despu√©s de resumir, volvemos al asistente\n",
        "\n",
        "# Compilaci√≥n final\n",
        "graph = builder.compile()\n",
        "\n",
        "print(\"‚úÖ Grafo con ciclo de memoria compilado correctamente.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cMTroSW3Iu9",
        "outputId": "4e29e5a0-9382-4412-8e1c-6ebf5973adb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Respuesta Final del Asistente:\n",
            "El estado del pedido ABC123 es: En preparaci√≥n.\n",
            "\n",
            "==============================\n",
            "\n",
            "üß† Memoria a Largo Plazo (Summary) Generada:\n",
            "Estado del pedido ABC123: En preparaci√≥n.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# 1. Definimos una consulta que obligue a usar una herramienta\n",
        "# (Esto activar√° el camino: Assistant -> Tools -> Memory -> Assistant)\n",
        "inputs = {\n",
        "    \"messages\": [HumanMessage(content=\"Hola, ¬øpodr√≠as verificar el estado del pedido ABC123?\")],\n",
        "    \"summary\": \"\"  # Empezamos con la memoria vac√≠a\n",
        "}\n",
        "\n",
        "# 2. Ejecutamos el grafo\n",
        "# recursion_limit es una seguridad para evitar bucles infinitos si la l√≥gica fallara\n",
        "result = graph.invoke(inputs, config={\"recursion_limit\": 10})\n",
        "\n",
        "# 3. Inspeccionamos los resultados\n",
        "print(f\"ü§ñ Respuesta Final del Asistente:\\n{result['messages'][-1].content}\")\n",
        "print(\"\\n\" + \"=\"*30 + \"\\n\")\n",
        "print(f\"üß† Memoria a Largo Plazo (Summary) Generada:\\n{result.get('summary', '‚ö†Ô∏è No se gener√≥ resumen')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "id": "nYsFdgG73qwf",
        "outputId": "6e1e2f9d-23c2-41df-95e3-4f743cf639b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2490267038.py:65: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(height=500, label=\"Conversaci√≥n\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://383e0f962055a8e0ca.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://383e0f962055a8e0ca.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://383e0f962055a8e0ca.gradio.live\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
        "\n",
        "def format_chat_history(messages):\n",
        "    \"\"\"Convierte el historial de LangChain a formato de chat de Gradio [[user, bot], ...]\"\"\"\n",
        "    history = []\n",
        "    temp_user_msg = None\n",
        "\n",
        "    for msg in messages:\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            temp_user_msg = msg.content\n",
        "        elif isinstance(msg, AIMessage) and msg.content:\n",
        "            # Solo mostramos respuestas que tengan contenido de texto (ignoramos tool calls vac√≠os)\n",
        "            if temp_user_msg:\n",
        "                history.append([temp_user_msg, msg.content])\n",
        "                temp_user_msg = None\n",
        "            else:\n",
        "                # Caso raro: AI habla sin prompt humano inmediato (continuaci√≥n)\n",
        "                history.append([None, msg.content])\n",
        "    return history\n",
        "\n",
        "def extract_debug_info(messages):\n",
        "    \"\"\"Busca qu√© herramientas se usaron en la √∫ltima interacci√≥n\"\"\"\n",
        "    tools_used = []\n",
        "    # Recorremos hacia atr√°s buscando ToolMessages recientes\n",
        "    for msg in reversed(messages):\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            break # Paramos al llegar al √∫ltimo input del usuario\n",
        "        if isinstance(msg, ToolMessage):\n",
        "            tools_used.append(f\"üîß {msg.name} (ID: {msg.tool_call_id[:5]}...)\")\n",
        "            # Opcional: Mostrar el resultado de la tool\n",
        "            # tools_used.append(f\"   -> Resultado: {msg.content[:50]}...\")\n",
        "\n",
        "    if not tools_used:\n",
        "        return \"No se usaron herramientas en este turno.\"\n",
        "    return \"\\n\".join(reversed(tools_used))\n",
        "\n",
        "def run_agent(input_text, state):\n",
        "    # 1. Inicializar estado si est√° vac√≠o\n",
        "    if not state:\n",
        "        state = {\"messages\": [], \"summary\": \"\"}\n",
        "\n",
        "    # 2. Agregar mensaje del usuario\n",
        "    state[\"messages\"].append(HumanMessage(content=input_text))\n",
        "\n",
        "    # 3. Invocar el Grafo\n",
        "    # recursion_limit alto por si el agente entra en bucles de pensamiento\n",
        "    result = graph.invoke(state, config={\"recursion_limit\": 10})\n",
        "\n",
        "    # 4. Procesar salidas para la UI\n",
        "    chat_history = format_chat_history(result[\"messages\"])\n",
        "    tools_log = extract_debug_info(result[\"messages\"])\n",
        "    current_summary = result.get(\"summary\", \"Sin resumen a√∫n.\")\n",
        "\n",
        "    return chat_history, result, tools_log, current_summary\n",
        "\n",
        "# --- Interfaz Gradio ---\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as ui:\n",
        "    gr.Markdown(\"# ü§ñ Agente ReAct con Memoria y RAG\")\n",
        "    gr.Markdown(\"Prueba interactiva del TP Final - UT4\")\n",
        "\n",
        "    with gr.Row():\n",
        "        # Columna Izquierda: Chat\n",
        "        with gr.Column(scale=2):\n",
        "            chatbot = gr.Chatbot(height=500, label=\"Conversaci√≥n\")\n",
        "            msg = gr.Textbox(label=\"Tu mensaje\", placeholder=\"Pregunt√° por pedidos (ABC123) o sobre LangGraph...\")\n",
        "            clear = gr.Button(\"Limpiar Conversaci√≥n\")\n",
        "\n",
        "        # Columna Derecha: Debug y Estado Interno\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### üß† Estado Interno del Agente\")\n",
        "            tools_output = gr.Textbox(label=\"Tools Usadas (√öltimo Turno)\", interactive=False)\n",
        "            summary_output = gr.TextArea(label=\"Memoria a Largo Plazo (Summary)\", interactive=False)\n",
        "\n",
        "            # Bot√≥n para inspeccionar el estado crudo (opcional)\n",
        "            state_display = gr.JSON(label=\"Estado Crudo (Full State)\", visible=False)\n",
        "\n",
        "    # Estado de la sesi√≥n (persiste entre interacciones)\n",
        "    agent_state = gr.State()\n",
        "\n",
        "    # Eventos\n",
        "    msg.submit(\n",
        "        run_agent,\n",
        "        inputs=[msg, agent_state],\n",
        "        outputs=[chatbot, agent_state, tools_output, summary_output]\n",
        "    )\n",
        "\n",
        "    # Limpiar input despu√©s de enviar\n",
        "    msg.submit(lambda: \"\", None, msg)\n",
        "\n",
        "    # Bot√≥n de limpieza\n",
        "    def clear_memory():\n",
        "        return None, [], \"\", \"Memoria reiniciada.\"\n",
        "\n",
        "    clear.click(clear_memory, outputs=[agent_state, chatbot, tools_output, summary_output])\n",
        "\n",
        "ui.launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9d6vPnH4g6F",
        "outputId": "702dcb4e-960c-4b39-d183-765b53268544"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üèÅ INICIANDO PRUEBAS DEL DESAF√çO INTEGRADOR üèÅ\n",
            "\n",
            "--- PRUEBA 1. Consulta RAG: ¬øC√≥mo se aprueba el curso? ---\n",
            "üîß Tool invocada: consultar_reglamento\n",
            "ü§ñ Respuesta Final: Para aprobar el curso, se requiere un portafolio de proyectos que representa el 60% de la evaluaci√≥n y una defensa oral final que cuenta por el 10%. Adem√°s, es necesario obtener una calificaci√≥n de B o superior para aprobar directamente.\n",
            "\n",
            "========================================\n",
            "\n",
            "--- PRUEBA 2. Consulta Estado: ¬øCu√°l es la situaci√≥n del alumno A002? ---\n",
            "üîß Tool invocada: ver_estado_alumno\n",
            "ü§ñ Respuesta Final: El alumno A002, Beto, se encuentra en estado condicional y tiene 2 pendientes.\n",
            "\n",
            "========================================\n",
            "\n",
            "--- PRUEBA 3. Mixta: Soy el alumno A001. ¬øTengo entregas pendientes? Y recordame cu√°ndo es la defensa final. ---\n",
            "üîß Tool invocada: ver_estado_alumno\n",
            "ü§ñ Respuesta Final: Como alumno A001, no tienes entregas pendientes y tu estado es regular. La defensa final est√° programada para el 02/12. La evaluaci√≥n del curso consiste en un portafolio de proyectos (60%) y una defensa oral final (10%).\n",
            "\n",
            "========================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import operator\n",
        "from typing import Optional, List, Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "# --- 1. Configuraci√≥n del Dominio (RAG) ---\n",
        "# Corpus de conocimiento (FAQs del curso basadas en tus PDFs)\n",
        "texts_curso = [\n",
        "    \"La evaluaci√≥n del curso consiste en un portafolio de proyectos (60%) y una defensa oral final (10%).\",\n",
        "    \"Para la aprobaci√≥n directa se requiere una calificaci√≥n de B o superior.\",\n",
        "    \"El curso cubre unidades de Machine Learning Cl√°sico, Deep Learning, Computer Vision, NLP y MLOps.\",\n",
        "    \"Las herramientas utilizadas incluyen Python, Pandas, Scikit-learn, PyTorch y LangChain.\",\n",
        "    \"La metodolog√≠a es aprendizaje basado en equipos y proyectos, con preparaci√≥n previa (Flipped Classroom).\",\n",
        "    \"La defensa final del curso est√° programada para el 02/12.\",\n",
        "    \"El profesor a cargo es el Ing. Juan Francisco Kurucz.\"\n",
        "]\n",
        "\n",
        "# Construcci√≥n del √≠ndice vectorial\n",
        "docs = [Document(page_content=t) for t in texts_curso]\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
        "chunks = splitter.split_documents(docs)\n",
        "vectorstore = FAISS.from_documents(chunks, OpenAIEmbeddings())\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# --- 2. Definici√≥n de Tools ---\n",
        "\n",
        "@tool\n",
        "def consultar_reglamento(pregunta: str) -> str:\n",
        "    \"\"\"Usa esta tool para responder dudas sobre el funcionamiento, evaluaci√≥n, fechas o contenido del curso.\"\"\"\n",
        "    docs = retriever.invoke(pregunta)\n",
        "    return \"\\n\".join([d.page_content for d in docs]) if docs else \"No encontr√© informaci√≥n en el reglamento.\"\n",
        "\n",
        "# Base de datos ficticia de alumnos\n",
        "DB_ALUMNOS = {\n",
        "    \"A001\": {\"nombre\": \"Ana\", \"estado\": \"Regular\", \"entregas_pendientes\": 0},\n",
        "    \"A002\": {\"nombre\": \"Beto\", \"estado\": \"Condicional\", \"entregas_pendientes\": 2},\n",
        "}\n",
        "\n",
        "@tool\n",
        "def ver_estado_alumno(matricula: str) -> str:\n",
        "    \"\"\"Consulta el estado acad√©mico de un alumno dado su ID de matr√≠cula (ej: A001).\"\"\"\n",
        "    alumno = DB_ALUMNOS.get(matricula)\n",
        "    if not alumno:\n",
        "        return \"Matr√≠cula no encontrada.\"\n",
        "    return f\"Alumno: {alumno['nombre']} | Estado: {alumno['estado']} | Pendientes: {alumno['entregas_pendientes']}\"\n",
        "\n",
        "tools = [consultar_reglamento, ver_estado_alumno]\n",
        "\n",
        "# --- 3. Definici√≥n del Estado y Grafo ---\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[list, operator.add]\n",
        "    summary: Optional[str]\n",
        "\n",
        "# Modelo y Nodos\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0).bind_tools(tools)\n",
        "\n",
        "def assistant_node(state: AgentState) -> AgentState:\n",
        "    # Inyectamos contexto de memoria si existe\n",
        "    msgs = state[\"messages\"]\n",
        "    if state.get(\"summary\"):\n",
        "        msgs = [SystemMessage(content=f\"Resumen previo: {state['summary']}\")] + msgs\n",
        "    return {\"messages\": [llm.invoke(msgs)]}\n",
        "\n",
        "def memory_node(state: AgentState) -> AgentState:\n",
        "    # Resumidor ligero\n",
        "    summary_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "    last_msgs = state[\"messages\"][-4:] # Miramos lo reciente\n",
        "    conversation = \"\\n\".join([f\"{m.type}: {m.content}\" for m in last_msgs])\n",
        "    curr_summary = state.get(\"summary\") or \"\"\n",
        "\n",
        "    prompt = f\"Actualiza este resumen: '{curr_summary}' con esto nuevo: '{conversation}'. S√© breve.\"\n",
        "    new_summary = summary_llm.invoke(prompt).content\n",
        "    return {\"summary\": new_summary}\n",
        "\n",
        "def router(state: AgentState) -> str:\n",
        "    last = state[\"messages\"][-1]\n",
        "    if last.tool_calls:\n",
        "        return \"tools\"\n",
        "    return END\n",
        "\n",
        "# Armado del Grafo\n",
        "builder = StateGraph(AgentState)\n",
        "builder.add_node(\"assistant\", assistant_node)\n",
        "builder.add_node(\"tools\", ToolNode(tools))\n",
        "builder.add_node(\"memory\", memory_node)\n",
        "\n",
        "builder.add_edge(START, \"assistant\")\n",
        "builder.add_conditional_edges(\"assistant\", router, {\"tools\": \"tools\", END: END})\n",
        "builder.add_edge(\"tools\", \"memory\")\n",
        "builder.add_edge(\"memory\", \"assistant\")\n",
        "\n",
        "agent = builder.compile()\n",
        "\n",
        "# --- 4. Ejecuci√≥n de las 3 Pruebas ---\n",
        "\n",
        "casos_prueba = [\n",
        "    \"1. Consulta RAG: ¬øC√≥mo se aprueba el curso?\",\n",
        "    \"2. Consulta Estado: ¬øCu√°l es la situaci√≥n del alumno A002?\",\n",
        "    \"3. Mixta: Soy el alumno A001. ¬øTengo entregas pendientes? Y recordame cu√°ndo es la defensa final.\"\n",
        "]\n",
        "\n",
        "print(\"üèÅ INICIANDO PRUEBAS DEL DESAF√çO INTEGRADOR üèÅ\\n\")\n",
        "\n",
        "for i, prompt in enumerate(casos_prueba):\n",
        "    print(f\"--- PRUEBA {prompt} ---\")\n",
        "    inputs = {\"messages\": [HumanMessage(content=prompt)], \"summary\": \"\"}\n",
        "\n",
        "    # Ejecutamos\n",
        "    for event in agent.stream(inputs):\n",
        "        for node_name, value in event.items():\n",
        "            if node_name == \"assistant\":\n",
        "                last_msg = value[\"messages\"][-1]\n",
        "                if last_msg.tool_calls:\n",
        "                    print(f\"üîß Tool invocada: {last_msg.tool_calls[0]['name']}\")\n",
        "                else:\n",
        "                    print(f\"ü§ñ Respuesta Final: {last_msg.content}\")\n",
        "    print(\"\\n\" + \"=\"*40 + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
