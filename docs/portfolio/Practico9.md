
# CNN vs Transfer Learning (CIFAR-10)

En este proyecto compar√© dos estrategias fundamentales de Deep Learning para visi√≥n artificial: **CNNs entrenadas desde cero** vs **Transfer Learning**. Demostr√© mi capacidad para:

- Dise√±ar arquitecturas CNN optimizadas para im√°genes de baja resoluci√≥n (32x32).
- Implementar Transfer Learning con modelos estado del arte (EfficientNetB0).
- Aplicar data augmentation para mejorar la robustez de los modelos.
- Realizar an√°lisis cr√≠tico de cu√°ndo Transfer Learning es apropiado.
- Evaluar trade-offs entre complejidad arquitect√≥nica y rendimiento.

Este proyecto revela insights importantes sobre la compatibilidad entre modelos pre-entrenados y dominios espec√≠ficos.

---

## Objetivos

En este proyecto me propuse: para la clasificaci√≥n de im√°genes en el dataset CIFAR-10:  
(1) una Red Neuronal Convolucional (CNN) simple entrenada desde cero, y  
(2) un modelo de vanguardia (EfficientNetB0) aplicando Transfer Learning.

El objetivo es evaluar y contrastar el rendimiento, la eficiencia del entrenamiento y el impacto de la arquitectura, demostrando la importancia de seleccionar la estrategia de modelo adecuada seg√∫n el dominio del problema.

---

## Objetivos

- Implementar CNNs usando TensorFlow/Keras para clasificaci√≥n de im√°genes.  
- Aplicar Transfer Learning (Extracci√≥n de Caracter√≠sticas y Fine-Tuning) con modelos pre-entrenados de Keras Applications.  
- Procesar datasets de im√°genes (CIFAR-10) utilizando `ImageDataGenerator` para el aumento de datos.  
- Evaluar modelos usando m√©tricas de clasificaci√≥n (accuracy, F1-score).  
- Comparar las arquitecturas CNN vs Transfer Learning en un problema de im√°genes de baja resoluci√≥n.

---

## Metodolog√≠a

### Preprocesamiento

- **Dataset:** `keras.datasets.cifar10` (50,000 im√°genes de entrenamiento, 10,000 de test).  
- **Clases:** 10 (airplane, automobile, bird, etc.).  
- **Dimensiones:** 32x32x3.  
- **Normalizaci√≥n:** Escalado de p√≠xeles (0‚Äì255) ‚Üí (0‚Äì1).  
- **Codificaci√≥n:** Etiquetas en formato one-hot encoding.  
- **Batch Size:** 128.

---

### Arquitectura 1: CNN Simple (Desde Cero)

Modelo secuencial optimizado para inputs de 32x32:

1. `Conv2D(32, (3, 3), activation='relu')`  
2. `MaxPooling2D((2, 2))`  
3. `Conv2D(64, (3, 3), activation='relu')`  
4. `MaxPooling2D((2, 2))`  
5. `Flatten()`  
6. `Dense(512, activation='relu')`  
7. `Dense(10, activation='softmax')` (Clasificador)  

**Par√°metros entrenables:** 2,122,186.

---

### Arquitectura 2: Transfer Learning (EfficientNetB0)

- **Base:** `applications.EfficientNetB0` (pre-entrenado en ImageNet, `include_top=False`, `pooling='avg'`).  
- **Cabezal:** `BatchNormalization()` ‚Üí `Dense(256, 'relu')` ‚Üí `Dropout(0.5)` ‚Üí `Dense(10, 'softmax')`.  
- **Par√°metros totales:** 4,385,197.

---

### Pipeline de Augmentation

Se utiliz√≥ `ImageDataGenerator` para ambos modelos a fin de mejorar la robustez y asegurar una comparaci√≥n justa.

### Transformaciones aplicadas:
- rotation_range = 15
- width_shift_range = 0.1
- height_shift_range = 0.1
- horizontal_flip = True
- zoom_range = 0.1


---

### Entrenamiento y Evaluaci√≥n

- **Optimizador:** `AdamW` (LR = 0.001).  
- **Loss:** `categorical_crossentropy`.  
- **Callbacks:** `EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)`.

**CNN Simple:**  
Entrenamiento est√°ndar (10 √©pocas) usando `datagen.flow()`.

**Transfer Learning (2 Etapas):**

1. **Etapa 1 ‚Äì Extracci√≥n:**  
   - Base congelada (`base_model.trainable = False`).  
   - Entrenamiento solo del cabezal (10 √©pocas, detenida en la 5).  
   - Par√°metros entrenables: 333,066.  

2. **Etapa 2 ‚Äì Fine-Tuning:**  
   - Se descongelaron las √∫ltimas 10 capas.  
   - LR reducido a `1e-4`.  
   - Entrenamiento por 10 √©pocas, detenido en la 8.

---

## Resultados Principales

| Modelo | val_accuracy | Observaciones |
|:--|:--:|:--|
| CNN Simple | **71.40%** | Buen rendimiento, convergencia estable, sin overfitting. |
| EfficientNetB0 (TL) | **14.56%** | Fallo total de aprendizaje; accuracy cercana al azar. |

**An√°lisis del fallo:**
- Etapa 1: `val_accuracy` m√°xima 13.37%.  
- Etapa 2: Divergencia del modelo (`val_loss` hasta 4.88).  
- F1-scores nulos (0.00) en 5 de 10 clases para TL.  
- CNN simple mostr√≥ F1-scores s√≥lidos (ej. 0.82 para "automobile").  

La CNN simple fue **56.84% superior** al enfoque de Transfer Learning.

---

## Conclusiones

- Contrario a lo esperado, Transfer Learning (EfficientNetB0) tuvo un rendimiento dr√°sticamente inferior (14.56%) que la CNN simple (71.40%).  
- El fallo sugiere incompatibilidad entre el modelo pre-entrenado y el dataset.  
- **Hip√≥tesis:** EfficientNetB0 (entrenado en ImageNet, con im√°genes grandes y detalladas) no transfiere bien a CIFAR-10 (im√°genes peque√±as, 32x32).  
- En este caso, una CNN dise√±ada espec√≠ficamente para las dimensiones del problema super√≥ al modelo complejo.

---

## üìì Notebook

[Ver Notebook Completo](UT3/Practico9.ipynb)

## Reflexi√≥n Personal

Esta pr√°ctica demuestra que el Transfer Learning no es una "bala de plata".  
Es esencial validar que la arquitectura pre-entrenada sea adecuada al dominio y las dimensiones del dataset.  

Un modelo m√°s complejo (como EfficientNetB0) no garantiza mejores resultados si el contexto de los datos es distinto (ImageNet vs. CIFAR-10).

---

## Pr√≥ximo Paso

- Probar arquitecturas m√°s ligeras (ej. MobileNetV2).  
- Intent√© aplicar el modelo a otro tipo de dataset, pero surgieron muchos problemas con los pesos preentrenados: incompatibilidades de tama√±o/escala de entrada y comportamiento inestable al cargar los pesos, lo que impidi√≥ un reentrenamiento fiable.  
- Analizar el impacto de las capas `BatchNormalization` durante la congelaci√≥n, que podr√≠an haber agravado la inestabilidad estad√≠stica con datos muy distintos a ImageNet.
