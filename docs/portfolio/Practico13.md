

# Fine-Tuning de Transformers para Sentimiento Financiero

En este proyecto demostr茅 mi dominio de **Transfer Learning con Transformers** aplic谩ndolo a NLP financiero. Traduje una necesidad organizacional (medir sentimiento de mercado) en una soluci贸n t茅cnica end-to-end:

- **Baseline robusto**: Implement茅 TF-IDF + Regresi贸n Log铆stica para establecer benchmark.
- **Fine-tuning comparativo**: Experiment茅 con modelo gen茅rico (BERT) vs especializado (FinBERT).
- **Diagn贸stico de overfitting**: Analis茅 curvas de entrenamiento y estabilidad del modelo.
- **Visualizaci贸n de embeddings**: Us茅 UMAP para demostrar c贸mo Transformers capturan sem谩ntica vs TF-IDF.
- **Manejo de desbalance**: Implement茅 WeightedTrainer con class weights para mejorar F1-macro.

Este proyecto demuestra c贸mo la especializaci贸n de dominio impacta en el rendimiento de modelos de lenguaje.

## Objetivos
- Traducir una necesidad organizacional (an谩lisis de sentimiento) en una soluci贸n t茅cnica de NLP.
- Implementar y evaluar un *baseline* estad铆stico (TF-IDF + Regresi贸n Log铆stica) para establecer un benchmark de rendimiento.
- Aplicar *fine-Tuning* (transfer learning) para especializar modelos Transformer preentrenados (`bert-base-uncased` y `ProsusAI/finbert`) en un dataset de dominio.
- Comparar arquitecturas (gen茅rica vs. espec铆fica de dominio) para seleccionar el modelo con el mejor balance de precisi贸n, velocidad y estabilidad.
- Diagnosticar el *overfitting* analizando las curvas de p茅rdida (Training vs. Validation).
- Demostrar visualmente (con UMAP) la superioridad del espacio de caracter铆sticas (embeddings) de un Transformer frente a TF-IDF.
- Evaluar t茅cnicas avanzadas (balanceo de clases) y medir su impacto real en el rendimiento.

## Metodolog铆a

### 1. An谩lisis Exploratorio (EDA) y Baseline
- **Problema**: Se parti贸 de la necesidad de clasificar el sentimiento de ~12k tweets financieros (Dataset: `zeroshot/twitter-financial-news-sentiment`).
- **An谩lisis de Datos**: El EDA revel贸 un **severo desbalance de clases**, con la clase "Neutral" (2) dominando sobre "Bullish" (1) y "Bearish" (0). Esto justific贸 el uso de **F1-macro** como m茅trica principal.
- **Baseline Cl谩sico**: Se implement贸 un pipeline de `TF-IDF` (con n-gramas 1,2) y `LogisticRegression`. Fue crucial usar `class_weight="balanced"` para forzar al modelo a prestar atenci贸n a las clases minoritarias.
- **Diagn贸stico Baseline**: Una proyecci贸n UMAP sobre las *features* de TF-IDF mostr贸 un "blob" ca贸tico, donde las tres clases estaban completamente mezcladas, prediciendo un rendimiento pobre.

### 2. Fine-Tuning de Transformer Gen茅rico
- **Objetivo**: Superar el *baseline* estad铆stico usando un modelo que entienda el contexto sem谩ntico.
- **Modelo**: Se seleccion贸 `bert-base-uncased`, un modelo gen茅rico preentrenado en texto general.
- **Entrenamiento**: Se aplic贸 *fine-tuning* usando el `Trainer` de Hugging Face por 6 茅pocas. Se monitorearon las m茅tricas de validaci贸n por 茅poca.

### 3. Fine-Tuning de Transformer de Dominio (Extensi贸n)
- **Hip贸tesis**: Un modelo preentrenado en texto financiero (`ProsusAI/finbert`) deber铆a superar al modelo gen茅rico.
- **Experimento**: Se repiti贸 el *fine-Tuning* con `ProsusAI/finbert` por 3 茅pocas (esperando una convergencia m谩s r谩pida).
- **An谩lisis**: Se compar贸 el F1-macro, el tiempo de entrenamiento y las curvas de p茅rdida contra el modelo gen茅rico.

### 4. An谩lisis de Espacio Latente (Extensi贸n)
- **Objetivo**: Demostrar *visualmente* por qu茅 el Transformer supera al *baseline*.
- **T茅cnica**: Se extrajeron los *logits* (la capa de salida previa a la clasificaci贸n) del modelo `FinBERT` entrenado.
- **Visualizaci贸n**: Se aplic贸 UMAP a estos *logits* para proyectarlos en 2D y comparar la separabilidad de las clases contra el "blob" ca贸tico del TF-IDF.

### 5. Evaluaci贸n de Balanceo de Clases (Extensi贸n)
- **Hip贸tesis**: Dado que el F1 del *baseline* mejor贸 con el balanceo, aplicar esta t茅cnica al Transformer podr铆a mejorar a煤n m谩s el F1-macro.
- **T茅cnica**: Se cre贸 una subclase `WeightedTrainer` que sobreescribe `compute_loss`, aplicando pesos (`nn.CrossEntropyLoss(weight=...)`) para penalizar m谩s los errores en las clases minoritarias.
- **An谩lisis**: Se compar贸 el F1-macro final de `FinBERT + Balanced` contra `FinBERT` est谩ndar.

## Resultados Principales

### 1. Comparativa de Modelos (Baseline vs. Transformers)

El *fine-tuning* de Transformers demostr贸 un salto cu谩ntico en rendimiento, superando al *baseline* estad铆stico por ~10 puntos de F1-macro.

| Modelo | F1-Macro | Notas |
| :--- | ---: | :--- |
| Baseline (TF-IDF + LR Balanced) | 0.7321 | Techo de rendimiento estad铆stico. |
| **Gen茅rico (BERT-base)** | **0.8265** | **Mejor F1.** Logrado en la 茅poca 6. |
| Dominio (FinBERT - Sin Balanceo) | 0.8216 | F1 casi id茅ntico, pero m谩s eficiente. |
| Dominio (FinBERT + Balanced) | 0.8196 | El balanceo *empeor贸* el rendimiento. |

### 2. Diagn贸stico de Arquitecturas (Gen茅rico vs. Dominio)

El F1-score m谩s alto no cont贸 toda la historia. El an谩lisis de las curvas de entrenamiento revel贸 un claro ganador organizacional:

- **Gen茅rico (`bert-base-uncased`):** Logr贸 el F1 m谩s alto (0.8265), pero a un costo alto. El entrenamiento mostr贸 un **overfitting severo** despu茅s de la 茅poca 2 (Training Loss a 0.02, Validation Loss disparado de 0.37 a 0.68). El modelo estaba "memorizando".
- **Dominio (`ProsusAI/finbert`):** Logr贸 un F1 casi id茅ntico (0.8216) pero fue **m谩s eficiente y estable**. Alcanz贸 su rendimiento m谩ximo en **3 茅pocas** (6.5 min) vs 6 茅pocas (15.5 min) del gen茅rico, y sus curvas de p茅rdida fueron mucho m谩s saludables (sin overfitting severo).

**Conclusi贸n del Trade-off**: `FinBERT` es la elecci贸n superior para producci贸n, ofreciendo el mismo rendimiento con la mitad del costo de entrenamiento y mayor estabilidad.

### 3. Impacto Visual del Fine-Tuning (El "Blob" vs. los "Continentes")

La visualizaci贸n UMAP valid贸 por qu茅 los Transformers ganaron:
- **TF-IDF (Baseline)**: Mostr贸 un "blob" ca贸tico donde las clases 0, 1 y 2 eran indistinguibles.
- **FinBERT (Transformer)**: Mostr贸 tres "continentes" de clases claros y sem谩nticamente separados. El Transformer no "encontr贸" una estructura, **la cre贸**, fabricando un espacio de caracter铆sticas separable que explica el salto de 10 puntos en F1.

### 4. Resultado de T茅cnicas Avanzadas (El Balanceo Fall贸)

La hip贸tesis de la Extensi贸n 5 fall贸. Aplicar balanceo de clases al Transformer **perjudic贸** el rendimiento (F1 0.8216 -> 0.8196).

- **An谩lisis**: A diferencia del modelo estad铆stico, el Transformer (con su mecanismo de auto-atenci贸n) fue lo suficientemente robusto para manejar el desbalance de clases por s铆 mismo. La "sobre-correcci贸n" manual (forzar los pesos) desvi贸 al modelo y empeor贸 su capacidad de generalizaci贸n.

## Conclusiones
- El *fine-tuning* no es opcional, es un paso cr铆tico. Los modelos Transformer generaron **+10 puntos de F1-macro** sobre el *baseline* estad铆stico, demostrando el valor de entender el contexto sem谩ntico.
- El F1-score m谩s alto no es el "mejor" modelo. El gen茅rico `bert-base` (0.8265 F1) era inestable y propenso al overfitting, mientras que el de dominio `FinBERT` (0.8216 F1) fue la **opci贸n superior para producci贸n** (m谩s r谩pido, m谩s estable, mismo rendimiento).
- El EDA visual (UMAP) es clave para el diagn贸stico. Se demostr贸 visualmente que TF-IDF no pod铆a separar las clases ("blob"), mientras que el Transformer s铆 lo hizo ("continentes").
- No todas las t茅cnicas "avanzadas" ayudan. El balanceo de clases fue cr铆tico para el *baseline* simple, pero fue **contraproducente** para el Transformer avanzado, que ya manejaba el desbalance. Se debe medir y validar, no asumir.

## Reflexi贸n Personal
Esta pr谩ctica ejecut贸 un proyecto de NLP de extremo a extremo, desde la justificaci贸n del problema hasta la selecci贸n de un modelo listo para producci贸n. El proceso reflej贸 perfectamente el ciclo de vida de MLOps:
1.  Establecer un **Baseline** (TF-IDF) medible.
2.  Probar una soluci贸n moderna (**Transformers**) y demostrar su superioridad.
3.  **Diagnosticar** el entrenamiento (overfitting en `bert-base`).
4.  **Comparar trade-offs** (eficiencia y estabilidad de `FinBERT` vs. F1 marginal de `bert-base`).
5.  **Validar hip贸tesis** (el balanceo de clases fall贸).

---

##  Notebook

[Ver Notebook Completo](UT4/Practico13.ipynb)

