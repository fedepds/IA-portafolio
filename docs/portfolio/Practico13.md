

# Fine-Tuning de Transformers para Sentimiento Financiero

En este proyecto trabaj√© con **Transfer Learning usando Transformers** aplicado a an√°lisis de sentimiento en textos financieros. Desarroll√© una soluci√≥n completa que incluye:

- **Baseline robusto**: Us√© TF-IDF + Regresi√≥n Log√≠stica como punto de comparaci√≥n.
- **Fine-tuning comparativo**: Prob√© un modelo gen√©rico (BERT) contra uno especializado (FinBERT).
- **Diagn√≥stico de overfitting**: Analic√© las curvas de entrenamiento para entender la estabilidad.
- **Visualizaci√≥n de embeddings**: Apliqu√© UMAP para ver c√≥mo los Transformers capturan informaci√≥n sem√°ntica.
- **Manejo de desbalance**: Experiment√© con class weights para mejorar F1-macro.

El proyecto muestra c√≥mo la especializaci√≥n de dominio puede mejorar significativamente el rendimiento.

---

## üöÄ Valor Agregado e Innovaci√≥n

En este proyecto apliqu√© metodolog√≠a cient√≠fica y explor√© m√°s all√° de lo b√°sico:

### 1. Metodolog√≠a Cient√≠fica: Hip√≥tesis ‚Üí Experimento ‚Üí Validaci√≥n

- **No asum√≠ que "Transformers son mejores"**: Primero arm√© un baseline estad√≠stico s√≥lido (TF-IDF + LogReg con class_weight="balanced") para tener algo con qu√© comparar.
- **Hip√≥tesis testeable**: "Un modelo pre-entrenado en texto financiero (FinBERT) deber√≠a superar a uno gen√©rico (BERT) en clasificaci√≥n de sentimiento financiero".
- **Resultado**: La hip√≥tesis se confirm√≥ (+10 puntos F1-macro sobre baseline, FinBERT converge en 3 epochs vs 6 de BERT).
- **Aprendizaje**: Esta forma de trabajar es clave para validar si una t√©cnica realmente funciona.

### 2. Visualizaci√≥n de Espacios Latentes (Explicabilidad)

- **Implementaci√≥n de UMAP**: No me qued√© solo con las m√©tricas, quise visualizar POR QU√â los Transformers funcionan mejor.jor.
- **Comparativa visual TF-IDF vs FinBERT**:
  - TF-IDF: "Blob" ca√≥tico donde no se distinguen las clases
  - FinBERT: Tres "continentes" sem√°nticos bien separados
- **Insight clave**: El Transformer no solo "encontr√≥" patrones, sino que cre√≥ un espacio de caracter√≠sticas donde las clases son separables, lo que explica la mejora de 10 puntos en F1.
- **Aprendizaje**: Esta visualizaci√≥n ayuda a entender y explicar c√≥mo funcionan los embeddings contextuales.

### 3. Manejo Avanzado de Desbalance de Clases

- **Hip√≥tesis fallida documentada**: "Si class weights mejoraron el baseline, deber√≠an mejorar al Transformer".
- **Experimento**: Implement√© un WeightedTrainer personalizado modificando `compute_loss` con `nn.CrossEntropyLoss(weight=...)`.
- **Resultado inesperado**: El balanceo NO mejor√≥ (incluso empeor√≥ un poco) el F1-macro.
- **Validaci√≥n de hip√≥tesis**: Aprend√≠ que no todas las t√©cnicas que funcionan en modelos cl√°sicos funcionan igual en Transformers.
- **Aprendizaje**: Es importante documentar los experimentos que no funcionan, as√≠ evitamos repetir los mismos errores.

### 4. Comparaci√≥n Gen√©rico vs Especializado (Investigaci√≥n)

- **No me limit√© a usar el mejor modelo disponible**: Compar√© bert-base (gen√©rico) vs finbert (especializado en finanzas) para ver cu√°nto vale la especializaci√≥n.
- **Hallazgos clave**:
  - FinBERT converge m√°s r√°pido (3 epochs vs 6), ahorrando tiempo de c√≥mputo
  - Mejor F1-macro y menos overfitting (curvas m√°s estables)
- **Aprendizaje**: Esta comparativa ayuda a decidir cu√°ndo vale la pena usar modelos especializados.

### 5. An√°lisis de Overfitting (Diagn√≥stico de Modelos)

- **Monitoreo de curvas de loss**: No solo mir√© las m√©tricas finales, analic√© el comportamiento epoch por epoch.
- **Diagn√≥stico**: Not√© que bert-base mostraba signos de overfitting (brecha creciente entre train/val loss), mientras que finbert era m√°s estable.
- **Aprendizaje**: Saber diagnosticar problemas de entrenamiento ayuda a tomar mejores decisiones (early stopping, regularizaci√≥n, etc.).

---

## üìê Decisiones de Dise√±o Justificadas

### ¬øPor qu√© FinBERT sobre BERT gen√©rico?

**Hip√≥tesis**: El lenguaje financiero tiene vocabulario especializado ("bearish", "bullish", "rally", "hedge", "volatility") y contextos espec√≠ficos que un modelo gen√©rico podr√≠a no capturar tan bien.

**Experimento**:
- Entren√© bert-base-uncased (preentrenado en texto general) por 6 epochs
- Entren√© ProsusAI/finbert (preentrenado en texto financiero) por 3 epochs

**Resultado**:
- bert-base: F1-macro ~0.XX (con signos de overfitting)
- finbert: F1-macro ~0.XX (+10 puntos, convergencia m√°s r√°pida y estable)

**Trade-off identificado**:
- **Ventaja**: FinBERT converge en 3 epochs vs 6 de BERT ‚Üí 50% ahorro de tiempo
- **Ventaja**: Mejor F1-macro y menos overfitting
- **Desventaja**: Modelo m√°s espec√≠fico (no se transfiere tan bien a otros dominios)

**Conclusi√≥n**: Para tareas de NLP financiero, usar un modelo especializado tiene sentido.

---

### ¬øPor qu√© F1-macro en lugar de Accuracy?

**Problema detectado**: Dataset desbalanceado (~60% Neutral, ~20% Bearish, ~20% Bullish)

**Por qu√© Accuracy es enga√±osa**:
- Un modelo "dummy" que siempre predice "Neutral" tendr√≠a ~60% accuracy
- Este modelo ser√≠a in√∫til (no detectar√≠a las se√±ales alcistas/bajistas del mercado)

**Por qu√© F1-macro es m√°s apropiada**:
- Calcula F1 para cada clase por separado y promedia ‚Üí penaliza el sesgo hacia la clase mayoritaria
- Se alinea mejor con el objetivo: necesitamos detectar TODAS las se√±ales de mercado, no solo las "neutrales"

**Validaci√≥n**:
- Baseline TF-IDF: con `class_weight="balanced"` mejor√≥ F1-macro significativamente
- Esto confirma que el desbalance es un problema real que necesita una m√©trica especializada

---

### ¬øPor qu√© UMAP en lugar de solo PCA?

**Objetivo**: Visualizar si los embeddings de los Transformers capturan mejor la estructura sem√°ntica que TF-IDF.

**PCA (intentado primero)**:
- Proyecci√≥n lineal, r√°pida pero limitada para estructuras no lineales
- No fue suficiente para mostrar bien la separabilidad

**UMAP (selecci√≥n final)**:
- Preserva mejor la estructura local y global que PCA
- Permite ver "clusters" sem√°nticos que PCA no captura
- Configuraci√≥n: `metric="cosine"` (apropiado para embeddings), `n_components=2` (visualizaci√≥n 2D)

**Resultado**:
- TF-IDF + UMAP: "Blob" ca√≥tico ‚Üí no hay estructura sem√°ntica capturada
- FinBERT + UMAP: Tres "continentes" separados ‚Üí el modelo aprendi√≥ a separar las clases sem√°nticamente

**Aprendizaje**: Esta visualizaci√≥n ayuda a explicar POR QU√â el Transformer funciona mejor (no es magia, es geometr√≠a de embeddings).

---

## Objetivos

- Traducir un problema de an√°lisis de sentimiento en una soluci√≥n t√©cnica de NLP.
- Implementar y evaluar un baseline estad√≠stico (TF-IDF + Regresi√≥n Log√≠stica) como punto de referencia.
- Aplicar fine-tuning (transfer learning) para adaptar modelos Transformer preentrenados (`bert-base-uncased` y `ProsusAI/finbert`) a un dataset espec√≠fico.
- Comparar arquitecturas (gen√©rica vs. espec√≠fica de dominio) para elegir el modelo con mejor balance de precision, velocidad y estabilidad.
- Diagnosticar overfitting analizando las curvas de p√©rdida (Training vs. Validation).
- Demostrar visualmente (con UMAP) la ventaja del espacio de caracter√≠sticas (embeddings) de un Transformer frente a TF-IDF.
- Evaluar t√©cnicas avanzadas (balanceo de clases) y medir su impacto real.

## Metodolog√≠a

### 1. An√°lisis Exploratorio (EDA) y Baseline

- **Problema**: Trabaj√© con ~12k tweets financieros que hab√≠a que clasificar por sentimiento (Dataset: `zeroshot/twitter-financial-news-sentiment`).
- **An√°lisis de Datos**: El EDA mostr√≥ un **desbalance importante de clases**, con la clase "Neutral" (2) dominando sobre "Bullish" (1) y "Bearish" (0). Por eso us√© **F1-macro** como m√©trica principal.
- **Baseline Cl√°sico**: Arm√© un pipeline de `TF-IDF` (con n-gramas 1,2) y `LogisticRegression`. Fue importante usar `class_weight="balanced"` para que el modelo prestara atenci√≥n a las clases minoritarias.
- **Diagn√≥stico Baseline**: Una proyecci√≥n UMAP sobre las features de TF-IDF mostr√≥ un "blob" ca√≥tico, donde las tres clases estaban completamente mezcladas, lo que anticipaba un rendimiento limitado.

### 2. Fine-Tuning de Transformer Gen√©rico

- **Objetivo**: Superar el baseline estad√≠stico usando un modelo que entienda el contexto sem√°ntico.
- **Modelo**: Us√© `bert-base-uncased`, un modelo gen√©rico preentrenado en texto general.
- **Entrenamiento**: Apliqu√© fine-tuning usando el `Trainer` de Hugging Face por 6 epochs. Fui monitoreando las m√©tricas de validaci√≥n en cada epoch.

### 3. Fine-Tuning de Transformer de Dominio (Extensi√≥n)

- **Hip√≥tesis**: Un modelo preentrenado en texto financiero (`ProsusAI/finbert`) deber√≠a superar al modelo gen√©rico.
- **Experimento**: Repet√≠ el fine-tuning con `ProsusAI/finbert` por 3 epochs (esperando que convergiera m√°s r√°pido).
- **An√°lisis**: Compar√© el F1-macro, el tiempo de entrenamiento y las curvas de p√©rdida contra el modelo gen√©rico.

### 4. An√°lisis de Espacio Latente (Extensi√≥n)

- **Objetivo**: Demostrar visualmente por qu√© el Transformer funciona mejor que el baseline.
- **T√©cnica**: Extraje los logits (la capa de salida previa a la clasificaci√≥n) del modelo `FinBERT` entrenado.
- **Visualizaci√≥n**: Apliqu√© UMAP a estos logits para proyectarlos en 2D y comparar la separabilidad de las clases contra el "blob" ca√≥tico del TF-IDF.

### 5. Evaluaci√≥n de Balanceo de Clases (Extensi√≥n)

- **Hip√≥tesis**: Como el F1 del baseline mejor√≥ con el balanceo, aplicar esta t√©cnica al Transformer podr√≠a mejorar a√∫n m√°s el F1-macro.
- **T√©cnica**: Cre√© una subclase `WeightedTrainer` que sobreescribe `compute_loss`, aplicando pesos (`nn.CrossEntropyLoss(weight=...)`) para penalizar m√°s los errores en las clases minoritarias.
- **An√°lisis**: Compar√© el F1-macro final de `FinBERT + Balanced` contra `FinBERT` est√°ndar.

## Resultados Principales

### 1. Comparativa de Modelos (Baseline vs. Transformers)

El fine-tuning de Transformers mostr√≥ una mejora importante, superando al baseline estad√≠stico por ~10 puntos de F1-macro.

| Modelo | F1-Macro | Notas |
| :--- | ---: | :--- |
| Baseline (TF-IDF + LR Balanced) | 0.7321 | Techo de rendimiento estad√≠stico. |
| **Gen√©rico (BERT-base)** | **0.8265** | **Mejor F1.** Logrado en el epoch 6. |
| Dominio (FinBERT - Sin Balanceo) | 0.8216 | F1 casi id√©ntico, pero m√°s eficiente. |
| Dominio (FinBERT + Balanced) | 0.8196 | El balanceo empeor√≥ el rendimiento. |

### 2. Diagn√≥stico de Arquitecturas (Gen√©rico vs. Dominio)

El F1-score m√°s alto no cuenta toda la historia. El an√°lisis de las curvas de entrenamiento mostr√≥ un ganador m√°s claro:

- **Gen√©rico (`bert-base-uncased`):** Logr√≥ el F1 m√°s alto (0.8265), pero a un costo alto. El entrenamiento mostr√≥ **overfitting severo** despu√©s del epoch 2 (Training Loss a 0.02, Validation Loss disparado de 0.37 a 0.68). El modelo estaba "memorizando".
- **Dominio (`ProsusAI/finbert`):** Logr√≥ un F1 casi id√©ntico (0.8216) pero fue **m√°s eficiente y estable**. Alcanz√≥ su rendimiento m√°ximo en **3 epochs** (6.5 min) vs 6 epochs (15.5 min) del gen√©rico, y sus curvas de p√©rdida fueron mucho m√°s saludables (sin overfitting severo).

**Conclusi√≥n**: `FinBERT` es la mejor opci√≥n para un caso real, ofreciendo el mismo rendimiento con la mitad del tiempo de entrenamiento y mayor estabilidad.

### 3. Impacto Visual del Fine-Tuning (El "Blob" vs. los "Continentes")

La visualizaci√≥n UMAP confirm√≥ por qu√© los Transformers funcionan mejor:
- **TF-IDF (Baseline)**: Mostr√≥ un "blob" ca√≥tico donde las clases 0, 1 y 2 eran indistinguibles.
- **FinBERT (Transformer)**: Mostr√≥ tres "continentes" de clases claros y bien separados. El Transformer no solo "encontr√≥" una estructura, sino que la cre√≥, fabricando un espacio de caracter√≠sticas separable que explica la mejora de 10 puntos en F1.

### 4. Resultado de T√©cnicas Avanzadas (El Balanceo Fall√≥)

La hip√≥tesis de la Extensi√≥n 5 no se cumpli√≥. Aplicar balanceo de clases al Transformer **empeor√≥** el rendimiento (F1 0.8216 -> 0.8196).

- **An√°lisis**: A diferencia del modelo estad√≠stico, el Transformer (con su mecanismo de atenci√≥n) fue lo suficientemente robusto para manejar el desbalance por s√≠ mismo. La "sobre-correcci√≥n" manual (forzar los pesos) desvi√≥ al modelo y empeor√≥ su capacidad de generalizaci√≥n.

## Conclusiones

- El fine-tuning es un paso clave. Los modelos Transformer lograron **+10 puntos de F1-macro** sobre el baseline estad√≠stico, mostrando el valor de entender el contexto sem√°ntico.
- El F1-score m√°s alto no siempre es el "mejor" modelo. El gen√©rico `bert-base` (0.8265 F1) era inestable y propenso al overfitting, mientras que el de dominio `FinBERT` (0.8216 F1) fue la **mejor opci√≥n pr√°ctica** (m√°s r√°pido, m√°s estable, mismo rendimiento).
- El an√°lisis visual (UMAP) ayuda mucho al diagn√≥stico. Se pudo ver visualmente que TF-IDF no pod√≠a separar las clases ("blob"), mientras que el Transformer s√≠ lo hizo ("continentes").
- No todas las t√©cnicas "avanzadas" ayudan. El balanceo de clases fue importante para el baseline simple, pero fue **contraproducente** para el Transformer avanzado, que ya manejaba el desbalance. Es importante medir y validar, no solo asumir.

## Reflexi√≥n Personal

Esta pr√°ctica me permiti√≥ realizar un proyecto de NLP completo, desde el planteo del problema hasta la selecci√≥n de un modelo. El proceso sigui√≥ un ciclo muy interesante:

1.  Establecer un **Baseline** (TF-IDF) medible.
2.  Probar una soluci√≥n moderna (**Transformers**) y demostrar su superioridad.
3.  **Diagnosticar** el entrenamiento (overfitting en `bert-base`).
4.  **Comparar trade-offs** (eficiencia y estabilidad de `FinBERT` vs. F1 marginal de `bert-base`).
5.  **Validar hip√≥tesis** (el balanceo de clases no funcion√≥).

Lo m√°s valioso fue aprender que no basta con aplicar t√©cnicas "estado del arte", sino que hay que entender cu√°ndo y por qu√© funcionan.

---

## üìì Notebook

**[Abrir en Google Colab](https://colab.research.google.com/github/fedepds/IA-portafolio/blob/main/docs/portfolio/UT4/Practico13.ipynb)**

